/**
 * 
 * About
 * 
 * 在cuda外围打转很长时间，关于架构的知识不打算再纠结下去，从今天开始切入代码学习。
 * 主要方法是通过重写cuda samples 的同时google代码解析，同时学习架构。
 * 
 * chunk 2014
 * 
**/


//0902

今天第一个例程是asyncAPI，网上没有找到解析。
不过程序readme中给出：This sample uses CUDA streams and events to overlap execution on CPU and GPU.
// This sample illustrates the usage of CUDA events for both GPU timing and
// overlapping CPU and GPU execution.  Events are insterted into a stream
// of CUDA calls.  Since CUDA stream calls are asynchronous, the CPU can
// perform computations while GPU is executing (including DMA memcopies
// between the host and device).  CPU can query CUDA events to determine
// whether GPU has completed tasks.


这个程序很小，只有一个.cu文件。作为第一个例程，makefile和程序的结构是重点。

0.makefile
	$^ 所有的依赖目标的集合。以空格分隔。如果在依赖目标中有多个重复的,那个这个变量会去除重复的依赖目标,只保留一份。
	$@   表示规则中的目标文件集。在模式规则中,如果有多个目标,那么,"$@"就是匹配于目标中模式定义的集合。

	= 是最基本的赋值
	:= 是覆盖之前的值
	?= 是如果没有被赋值过就赋予等号后面的值
	+= 是添加等号后面的值

1.INCLUDES  := -I../../common/inc
2.cuda编程模型
《cuda编程模型》
	记住三个图：
	1.三级线程组织
	2.内存分配模型
	3.硬件(SM和内存)结构
	
	其中，内存分配模型是pull request，当block分配至SM之后，由硬件对request进行检查、分配。
3.cuda编译
(1)浅析CUDA编译流程与配置方法 (我们现在需要的是cuda程序的编译结构(例如.cu和cpp文件的综合处理))
	http://blog.csdn.net/shi06/article/details/5110017
	Nvcc是一种编译器驱动，通过命令行选项可以在不同阶段启动不同的工具完成编译工作，其目的在于隐藏了复杂的CUDA编译细节，并且它不是一个特殊的CUDA编译驱动而是在模仿一般的通用编译驱动如gcc，它接受一定的传统编译选项如宏定义，库函数路径以及编译过程控制等。CUDA程序编译的路径会因在编译选项时设置的不同CUDA运行模式而不同，如模拟环境的设置等。nvcc封装了四种内部编译工具，即在C:/CUDA/bin目录下的nvopencc(C:/CUDA/open64/bin)，ptxas，fatbin，cudafe

	1.首先是对输入的cu文件有一个预处理过程
		cudafe被称为CUDA frontend，会被调用两次，完成两个工作：一是将主机代码与设备代码分离，生成gpu文件，二是对gpu文件进行dead code analysis，传给nvopencc。 Nvopencc生成ptx文件传给ptxas，最后将cubin或ptx传给fatbin。
	2.同时，在编译阶段CUDA源代码对C语言所扩展的部分将被转成regular ANSI C的源文件，也就可以由一般的C编译器进行更多的编译和连接。
		也即是设备代码被编译成ptx（parallel thread execution）代码或二进制代码，host代码则以C文件形式输出，在编译时可将设备代码链接到所生成的host代码，将其中的cubin对象作为全局初始化数据数组包含进来
	Nvcc的各个编译阶段以及行为是可以通过组合输入文件名和选项命令进行选择的。它是不区分输入文件类型的，如object, library or resource files，仅仅把当前要执行编译阶段需要的文件传递给linker。

	
(2)参考 NVIDIA CUDA Compiler Driver NVCC 官方文档 - http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#axzz3CA54wmUH

	This compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file, and several of these steps are subtly different for different modes of CUDA compilation (such as compilation for device emulation, or the generation of device code repositories). It is the purpose of the CUDA compiler driver nvcc to hide the intricate details of CUDA compilation from developers.

	Compilation Phases
(3)Another Example ：

	http://stackoverflow.com/questions/9421108/g-nvcc-how-to-compile-cuda-code-then-link-it-to-a-g-c-project/9505239#9505239

		all: program
		program: cudacode.o
			g++ -o program -L/usr/local/cuda/lib64 -lcuda -lcudart main.cpp  cudacode.o 
		cudacode.o:
			nvcc -c -arch=sm_20 cudacode.cu 
		clean: rm -rf *o program
	
	暴力方式：
	
		all:
			nvcc cudafile.cu mainfile.cpp -o executable
		clean:
			rm -rf *.o
			
(4)(from http://stackoverflow.com/users/749748/harrism)
	A couple of additional notes:

	1.You don't need to compile your .cu to a .cubin or .ptx file. You need to compile it to a .o object file and then link it with the .o object files from your .cpp files compiled wiht g++.
	
	2.In addition to putting your cuda kernel code in cudaFunc.cu, you also need to put a C or C++ wrapper function in that file that launches the kernel (unless you are using the CUDA driver API, which is unlikely and not recommended). Also add a header file with the prototype of this wrapper function so that you can include it in your C++ code which needs to call the CUDA code. Then you link the files together using your standard g++ link line.

	http://stackoverflow.com/questions/9363827/building-gpl-c-program-with-cuda-module

(5)/**
看起来像是终级解决方案：
	http://stackoverflow.com/questions/9363827/building-gpl-c-program-with-cuda-module
	
	You don't need to compile everything with nvcc. Your guess that you can just compile your CUDA code with NVCC and leave everything else (except linking) is correct. Here's the approach I would use to start.

	1. Add a 1 new header (e.g. myCudaImplementation.h) and 1 new source file (with .cu extension, e.g. myCudaImplementation.cpp). The source file contains your kernel implementation as well as a (host) C wrapper function that invokes the kernel with the appropriate execution configuration (aka <<<>>>) and arguments. The header file contains the prototype for the C wrapper function. Let's call that wrapper function runCudaImplementation()

	2. I would also provide another host C function in the source file (with prototype in the header) that queries and configures the GPU devices present and returns true if it is successful, false if not. Let's call this function configureCudaDevice().

	3.Now in your original C code, where you would normally call your CPU implementation you can do this.
		// must include your new header
		#include "myCudaImplementation.h"

		// at app initialization
		// store this variable somewhere you can access it later
		bool deviceConfigured = configureCudaDevice;          
		...                             
		// then later, at run time
		if (deviceConfigured) 
			runCudaImplementation();
		else
			runCpuImplementation(); // run the original code
	
	4.(最重要的)Now, since you put all your CUDA code in a new .cu file, you only have to compile that file with nvcc. Everything else stays the same, except that you have to link in the object file that nvcc outputs. e.g.
 
		nvcc -c -o myCudaImplementation.o myCudaImplementation.cu <other necessary arguments>

	Then add myCudaImplementation.o to your link line (something like:) g++ -o myApp myCudaImplementation.o

	5.(补充)Got it working. One additional step others may need to do is add extern "C" { } around the wrapper method. This is probably obvious to linking veterans, though. –  emulcahy
	(再补充)You shouldn't have to use extern "C" unless you only have C linkage (e.g. calling it from a .c file). –  harrism
*/
(6)	补充：
	关于extern "C" {} http://xlhnuaa.blog.163.com/blog/static/17233660320124293147308/
	1. 使用extern和包含头文件来引用函数有什么区别呢？extern的引用方式比包含头文件要简洁得多！extern的使用方法是直接了当的，想引用哪个函数就用extern声明哪个函数。这大概是KISS原则的一种体现吧！这样做的一个明显的好处是，会加速程序的编译（确切的说是预处理）的过程，节省时间。在大型C程序编译过程中，这种差异是非常明显的。
	
	2. 　作为一种面向对象的语言，C++支持函数重载，而过程式语言C则不支持。函数被C++编译后在符号库中的名字与C语言的不同。例如，假设某个函数的原型为：voidfoo( int x, int y );该函数被C编译器编译后在符号库中的名字为_foo，而C++编译器则会产生像_foo_int_int之类的名字
	
	未加extern "C"声明时的连接方式 - 实际上，在连接阶段，连接器会从模块A生成的目标文件moduleA.obj中寻找_foo_int_int这样的符号！
	
	加extern "C"声明后的编译和连接方式 - 
	extern "C" int foo( int x, int y );
	在模块B的实现文件中仍然调用foo（2,3），其结果是：（1）模块A编译生成foo的目标代码时，没有对其名字进行特殊处理，采用了C语言的方式；（2）连接器在为模块B的目标代码寻找foo(2,3)调用时，寻找的是未经修改的符号名_foo。

	3.extern "C"通常的使用技巧

(7) 关于架构选项
	http://docs.nvidia.com/cuda/maxwell-compatibility-guide/#verifying-maxwell-compatibility-using-cuda-6-0

	Note that compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.

	The NVIDIA CUDA C compiler, nvcc, can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number. For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.0 (Maxwell) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.

	When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU's native cubin format before launching it. If neither is available, then the kernel launch will fail.



	注意：检测到capabilitie = 2.1 情况下，在eclispe中无论自己指定gencode如何，最终它都会自动添加-gencode arch=compute_20,code=sm_21选项。即，对于capabilitie = 2.1，今后我们编译时只需要指定-gencode arch=compute_20,code=sm_21 .
	如果只指定gpu不指定ptx，则 同样会有-gencode arch=compute_20,code=sm_21
	但是不能都不指定，否则会 -gencode arch=compute_10,code=sm_10
	

4.代码分析
	0.kernel <<<>>>
	核函数只能在主机端调用，调用时必须申明执行参数。调用形式如下：
		Kernel<<<Dg,Db, Ns, S>>>(param list);
			<<<>>>运算符对kernel函数完整的执行配置参数形式是<<<Dg, Db, Ns, S>>>

			Dg：用于定义整个grid的维度和尺寸，即一个grid有多少个block。为dim3类型。Dim3 Dg(Dg.x, Dg.y, 1)表示grid中每行有Dg.x个block，每列有Dg.y个block，第三维恒为1(目前一个核函数只有一个grid)。整个grid中共有Dg.x*Dg.y个block，其中Dg.x和Dg.y最大值为65535。
			
			Db：用于定义一个block的维度和尺寸，即一个block有多少个thread。为dim3类型。Dim3 Db(Db.x, Db.y, Db.z)表示整个block中每行有Db.x个thread，每列有Db.y个thread，高度为Db.z。Db.x和Db.y最大值为512，Db.z最大值为62。 一个block中共有Db.x*Db.y*Db.z个thread。计算能力为1.0,1.1的硬件该乘积的最大值为768，计算能力为1.2,1.3的硬件支持的最大值为1024。
			
			Ns：是一个可选参数，用于设置每个block除了静态分配的shared Memory以外，最多能动态分配的shared memory大小，单位为byte。不需要动态分配时该值为0或省略不写。
			
			S：是一个cudaStream_t类型的可选参数，初始值为零，表示该核函数处在哪个流之中。


	1. cudaError_t CUDARTAPI cudaMallocHost(void **ptr, size_t size);
	关于使用了cudaMallocHost之后发生 segment fault的原因
	由于cudaMallocHost或者其完全版本 cudaHostAlloc，他们都是在host side申请空间，所以所得到的指针都是属于主机端的，但是这个与使用标准库函数malloc／alloc等略有不同，因为使用cuda＊申请的空间的指针是由cuda来维护的，一般的，这里发生segment fault，一般都是由于相应指针定义失效，以至于相应的引用指向无效位置，而这种使之失活的操作（在我今天的错误当中）就是 cudaDeviceReset(void)；在cudaDeviceReset(void)之后出现的对于之前cudaMallocHost的指针，如果再引用，会导致无效引用，错误产生；

	2.struct __device_builtin__ cudaDeviceProp

	3.cudaGetDeviceProperties(&deviceProps, devID)

	4.定义于inc/helper_cuda.h:
		#define checkCudaErrors(val)           check ( (val), #val, __FILE__, __LINE__ )

	5.cudaEvent_t ： 事件
		cudaEvent_t start,stop;
		cudaEventCreate(&start);
		cudaEventCreate(&stop);
		cudaEventRecord(start,0);
		
		其中，cudaEventRecord是产生事件并记录（用于记录当前场景）

		使用事件来测量性能
        1. 用途：为了测量GPU在某个任务上花费的时间。CUDA中的事件本质上是一个GPU时间戳。由于事件是直接在GPU上实现的。因此不适用于对同时包含设备代码和主机代码的混合代码设计。
        2. 形式：首先创建一个事件，然后记录事件，再计算两个事件之差，最后销毁事件。如：		
			cudaEvent_t start, stop;
			cudaEventCreate( &start );
			cudaEventCreate( &stop );
			cudaEventRecord( start, 0 );
			//do something
			cudaEventRecord( stop, 0 );
			float   elapsedTime;
			cudaEventElapsedTime( &elapsedTime,start, stop );
			cudaEventDestroy( start );
			cudaEventDestroy( stop )；

	6.关于cutil.h 
	http://stackoverflow.com/questions/12474191/cuda5-examples-has-anyone-translated-some-cutil-definitions-to-cuda5
		被helper_*.h代替，如
		helper_cuda.h, helper_cuda_gl.h, helper_cuda_drvapi.h, helper_functions.h, helper_image.h, helper_math.h, helper_string.h, and helper_timer.h
		函数cutil* / cut* 被 sdk*取代。
		如 cutCreateTimer(&timer); 替换成 sdkCreateTimer(&timer); 


	7.cudaMemcpyAsync ： cuda流
	http://www.tuicool.com/articles/fY7ryi
		CUDA流表示一个GPU操作队列，并且该队列中的操作以添加到队列的先后顺序执行。使用CUDA流可以实现任务级的并行，比如当GPU在执行核函数的同时，还可以在主机和设备之间交换数据(前提是GPU支持重叠，property的deviceOverlay为true)。
		cudaMemcpyAsync只是放置一个请求，表示在流中执行一次内存复制操作。函数返回时，复制操作不一定启动或执行结束，只是该操作被放入执行队列，在下一个被放入流中的操作之前执行。
		cuda流的使用能提高程序的执行效率。原理主要是使数据复制操作和核函数执行操作交叉执行，不用等到第一次核函数执行结束再开始第二轮的数据复制，以减少顺序执行带来的延迟(类似于编译中使用流水线在解决冲突的前提下提高效率)。
		(为此我们可能需要有多个流，如streamTest.cu，其中有两行：
			kernel<<<N/256, 256, 0, stream0>>>(dev_a0, dev_b0, dev_c0);
			kernel<<<N/256, 256, 0, stream1>>>(dev_a1, dev_b1, dev_c1);
		)
		

5.users' word
	http://bookc.github.io/
	CUDA流表示一个GPU操作队列，并且该队列中的操作将以指定的顺序执行。我们可以在流中添加一些操作，如
	
		核函数启动，内存复制 以及 事件的启动和结束
		
	等。这些操作的添加到流的顺序也是它们的执行顺序。可以将每个流视为GPU上的一个任务，并且这些任务可以并行执行。

	流同步：通过cudaStreamSynchronize()来协调。


7.其他
	1.关于cudaMallocHost and cudaHostAlloc
	The difference between the two if you're compiling with nvcc: nothing. The only difference is if you're using the runtime API from a C program rather than a C++ program (nvcc always compiles as C++), and then cudaHostAlloc is how you specify flags for your allocation.

	2.关于syncronize （barrier）
		cudaStreamSynchronize vs CudaDeviceSynchronize vs cudaThreadSynchronize
		These are all barriers. Barriers prevent code execution beyond the barrier until some condition is met.

		cudaDeviceSynchronize() halts execution in the CPU/host thread (that the cudaDeviceSynchronize was issued in) until the GPU has finished processing all previously requested cuda tasks (kernels, data copies, etc.)
		
		cudaThreadSynchronize() as you've discovered, is just a deprecated version of cudaDeviceSynchronize. Deprecated just means that it still works for now, but it's recommended not to use it (use cudaDeviceSynchronize instead) and in the future, it may become unsupported. But cudaThreadSynchronize() and cudaDeviceSynchronize() are basically identical.
		
		cudaStreamSynchronize() is similar to the above two functions, but it prevents further execution in the CPU host thread until the GPU has finished processing all previously requested cuda tasks that were issued in the referenced stream. So cudaStreamSynchronize() takes a stream id as it's only parameter. cuda tasks issued in other streams may or may not be complete when CPU code execution continues beyond this barrier.


	3.计时
		
		/**
		 * timer utils
		 */
		#include <sys/time.h>
		static struct timeval _tstart, _tend;
		static struct timezone tz;
		void tstart(void) {
			gettimeofday(&_tstart, &tz);
		}
		void tend(void) {
			gettimeofday(&_tend, &tz);
		}
		double tval() {
			double t1, t2;
			t1 = (double) _tstart.tv_sec + (double) _tstart.tv_usec / (1000 * 1000);
			t2 = (double) _tend.tv_sec + (double) _tend.tv_usec / (1000 * 1000);
			return t2 - t1;
		}

		或者：
		#include <sys/time.h>
		class mTimer {
			struct timeval _tstart, _tend;
			struct timezone tz;
		public:
			void start() {
				gettimeofday(&_tstart, &tz);
			}
			void end() {
				gettimeofday(&_tend, &tz);
			}
			double getTime() {
				double t1, t2;
				t1 = (double) _tstart.tv_sec + (double) _tstart.tv_usec / (1000 * 1000);
				t2 = (double) _tend.tv_sec + (double) _tend.tv_usec / (1000 * 1000);
				return t2 - t1;
			}
			void showTime() {
				printf("time in seconds:\t%.12f s\n", getTime());
			}
		};

		或者：
		
		class cuTimer {
			StopWatchInterface *timer;
		public:
			cuTimer() {
				sdkCreateTimer(&timer);
			}
			~cuTimer() {
				sdkDeleteTimer(&timer);
			}
			void start() {
				sdkStartTimer(&timer);
			}
			void end() {
				sdkStopTimer(&timer);
			}
			double getTime() {
				return sdkGetTimerValue(&timer);
			}
			//Time in msec
			void showTime() {
				printf("time in msec:\t%.12f ms\n", getTime());
			}
		};


	4.关于生命周期！注意！
	除非显式cudaFree或者GPU重启，否则即使程序结束或者cudaDeviceReset也不能将显存中的数据清除。
	
	cudaDeviceReset : Explicitly destroys and cleans up all resources associated with the current device in the current process. Any subsequent API call to this device will reinitialize the device.
	Note that this function will reset the device immediately. It is the caller's responsibility to ensure that the device is not being accessed by any other host threads from the process when this function is called.
	
		其他方法：
		1.nvidia-smi -i 0 -r
			Resetting GPU is not supported on this device.
			Treating as warning and moving on.
			All done.
		
	5.TMD!随机数也有问题！尤其是在eclipse中，如果不设置srand，随机序列时不回变化的！
		惊出一身冷汗，这让我想到了我的密码学作业。。。

8.关于并发(concurrent)与并行(parallel)
	http://www.doc88.com/p-944523615763.html


注意：未解决问题 - reset


//0903

今天第二个例程是clock，因为CDP(cuda dynamic parallism)要求capabilitie 在3.5以上。

2_clock的目的是通过device端的timer计算精确的（不包括IO）的时钟时间，然后将这个时间传回host读取。
本程序涉及几个重要的话题。

1.__shared__
	http://stackoverflow.com/questions/5531247/allocating-shared-memory
	
	(1)
	__shared__ int a[count]; //error
	
	“const doesn't mean "constant", it means "read-only".”
	A constant expression is something whose value is known to the compiler at compile-time.
	(2)
	__shared__ int a[100];
    __shared__ int b[4];	//ok
	
	or
	
	extern __shared__ int *shared;
    int *a = &shared[0]; //a is manually set at the beginning of shared
    int *b = &shared[count_a]; //b is manually set at the end of a

	(3)
	extern __shared__ int a[]; //ok
	...
	Kernel<<< gridDim, blockDim, a_size >>>(count);
	
	CUDA supports dynamic shared memory allocation. If you declare the kernel like that。
	
	也就是说，cuda的动态分配是在kernel参数中体现的，Ds指定了多少，最终就一定会分配这么多，并且顺序供给，用不完也在那。
	
2.至于为什么动态分配时是extern，解释如下：
	(1)本意：生命某个全局变量存在于某个源文件的某处，让编译器放心
	http://stackoverflow.com/questions/10422034/when-to-use-extern-in-c
	extern在c语言中的本意：This comes in useful when you have global variables. You declare the existence of global variables in a header, so that each source file that includes the header knows about it, but you only need to “define” it once in one of your source files.
	
	To clarify, using extern int x; tells the compiler that an object of type int called x exists somewhere. It's not the compilers job to know where it exists, it just needs to know the type and name so it knows how to use it. 

	(2) 关于 extern "C"
	http://stackoverflow.com/questions/1041866/in-c-source-what-is-the-effect-of-extern-c
	Since C++ has overloading of function names and C does not, the C++ compiler cannot just use the function name as a unique id to link to, so it mangles the name by adding information about the arguments. A C compiler does not need to mangle the name since you can not overload function names in C. When you state that a function has extern "C" linkage in C++, the C++ compiler does not add argument/parameter type information to the name used for linkage.
	这里的目的不是欺骗编译器，而是指导编译器在编译过程中对这个保护区进行特殊处理。
	一种跨c/c++的编码方式：
		#ifdef __cplusplus
		extern "C" {
		#endif

		// all of your legacy C code here

		#ifdef __cplusplus
		}
		#endif
	
	(2) extern与头文件(*.h)的区别和联系
	http://lpy999.blog.163.com/blog/static/117372061201182051413310/
	http://blog.csdn.net/yuyantai1234/article/details/7245412
	
	首先说下头文件，其实头文件对计算机而言没什么作用，她只是在预编译时在#include的地方展开一下，没别的意义了，其实头文件主要是给别人看的。
	
	那既然是说明，那么头文件里面放的自然就是关于函数，变量，类的“声明”(对函数来说，也叫函数原型)了。记着，是“声明”，不是“定义”。那么，我假设大家知道声明和定义的区别。所以，最好不要傻嘻嘻的在头文件里定义什么东西。比如全局变量.
	
	这个关键字真的比较可恶，在定义变量的时候，这个extern居然可以被省略(定义时，默认均省略)；在声明变量的时候，这个extern必须添加在变量前，所以有时会让你搞不清楚到底是声明还是定义。或者说，变量前有extern不一定就是声明，而变量前无extern就只能是定义。注：定义要为变量分配内存空间；而声明不需要为变量分配内存空间。
	
	  函数，对于函数也一样，也是定义和声明，定义的时候用extern，说明这个函数是可以被外部引用的，声明的时候用extern说明这是一个声明。 但由于函数的定义和声明是有区别的，定义函数要有函数体，声明函数没有函数体(还有以分号结尾)，所以函数定义和声明时都可以将extern省略掉，反正其他文件也是知道这个函数是在其他地方定义的，所以不加extern也行。
	  
	/***
	 * 注意：通过上述可知，对于函数来说，关键字'extern'可有可无；对于变量来说，声明时必须有extern。
	 * 为统一起见，我们将全局函数和变量的声明都加上extern。
	 * 注意，对于变量来说，如果需要声明的时候同时定义（在同一source文件中），则直接定义，不加extern，这与我们原先的习惯不矛盾。
	 */
	
	发现了没有？从此可以省略.h文件了！extern可以看成精简版的胶水。
	
	
	
	同时需要注意：we should also mention that this means that if you have two kernels with shared memory, you should use different identifiers for the shared memory pointer. it's not completely necessary but can help avoid some errors.nvcc wouldn't compile when i had extern __shared__ int *shared in one kernel and extern __shared__ float *shared in another kernel 
	

3.同步函数__syncthreads()
	__syncthreads()是 CUDA 的内置命令，其作用是保证 block 内的所有线程都已经运行到调用__syncthreads()的位置，这样可以保证各个线程看到的存储器是一样的。其头文件为 device_functions.h
	
	http://www.cnblogs.com/dwdxdy/p/3215136.html
	一句话：块内线程同步
	
	
	
4.规约法

	参考官方文档 - 关于cuda规约优化 http://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
		by Mark Harris (http://www.markmark.net/ , http://stackoverflow.com/users/749748/harrism)

	部分翻译 ： http://hackecho.com/2013/04/cuda-parallel-reduction/

5.其他
	1.rand()
	h_A[i] = (int) ((float) NUM_THREADS * rand() / (RAND_MAX + 1.0));
	其中(float) 不能落下，否则全零。
	
	2.checkCudaErrors
	checkCudaErrors还是很有用的，比如用printf调试了半天无果，加上checkCudaErrors一检验，立马报错：CUDA error at ../src/clock.cu:76 code=77(<unknown>) "cudaMemcpy(h_timer, d_timer, NUM_BLOCKS * 2 * sizeof(clock_t), cudaMemcpyDeviceToHost)" 

	关于错误检查！！！！！坑爹！
	http://stackoverflow.com/questions/19172408/acudaerrorunknown-in-cudamemcpy-function-call
	The documentation for both cudaMemcpy and cudaFree contains the following note:

	Note that this function may also return error codes from previous, asynchronous launches.

	ie. the error isn't happening in either cudaMemcpy or cudaFree, rather it is happening during the previous kernel launch or execution. If you follow this advice and modify your code to something like this:

		vectorMPL<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_D, numElements);
		checkCudaErrors(cudaPeekAtLastError());
		checkCudaErrors(cudaDeviceSynchronize());
		
	You should find that the error is reported by the cudaDeviceSynchronize() call, indicating that the error occurred when kernel was executing. The underlying reason for the error will most likely be out of bounds memory access ... ...
	
	3.bug eye for this code.
	timer[bid + blockDim.x] = clock();	// HERE : bug eye!

6.还有个重要的问题未解决：测试最佳blocksize


	
事先一定要把结构图画清楚！！

//0904

今天第三个例程是cppIntegration，这也是阴影之一，力图破之。

readme很清楚: This example demonstrates how to integrate CUDA into an existing C++ application, i.e. the CUDA entry point on host side is only a function which is called from C++ code and only the file containing this function is compiled with nvcc. It also demonstrates that vector types can be used from cpp.

1.extern
	这个问题之前已经讨论过了。
2.关于bank conflicts
	// use integer arithmetic to process all four bytes with one thread
	// this serializes the execution, but is the simplest solutions to avoid
	// bank conflicts for this very low number of threads
	// in general it is more efficient to process each byte by a separate thread,
	// to avoid bank conflicts the access pattern should be
	// g_data[4 * wtid + wid], where wtid is the thread id within the half warp
	// and wid is the warp id
	// see also the programming guide for a more in depth discussion.
	看完 https://www.youtube.com/watch?v=CZgM3DEBplE 
	就明白了。
	首先，访存是按对齐的字的方式，因此以int方式会大大提高效率
	
	https://www.youtube.com/watch?v=CZgM3DEBplE  中同时给出了测试bank conflict的程序，过会儿可以试一下。
	
3.用int代char的方式
	这样进行批量处理有哪些优势和劣势？


//0904

今天第四个例程是cppOverload. readme中解释 ： This sample demonstrates how to use C++ function overloading on the GPU.

1.除法向上取整固定套路：
	#define DIV_UP(a, b) (((a) + (b) - 1) / (b))
	用在计算blocksize（即griddim）时 ： <<<DIV_UP(N, THREAD_N), THREAD_N>>>
2.再次讨论cudaDeviceReset
	// cudaDeviceReset causes the driver to clean up all state. While
	// not mandatory in normal operation, it is good practice.  It is also
	// needed to ensure correct operation when the application is being
	// profiled. Calling cudaDeviceReset causes all profile data to be
	// flushed before the application exits

3.shared_mem 和 L1_cache的分配
	cudaFuncSetCacheConfig(*func2, cudaFuncCachePreferShared)；


//0905

从今天开始，我们要简略的过几个0_sample中的samples，需要加快些脚步。。。

/**
 * cudaOpenMP
 */
	This sample demonstrates how to use OpenMP API to write an application for multiple GPUs.  This executable is not pre-built with the SDK installer.
1.-lgomp
	不知道为什么以前没有链接这个库没事，现在除了编译指定-fopenmp外，还必须链接这个库。
	位置/usr/lib/gcc/x86_64-linux-gnu/4.6/libgomp.so
	http://gcc.gnu.org/projects/gomp/ 
		GOMP — An OpenMP implementation for GCC - GNU Project

2.本程序实现的是多device(GPU)之间通过openmp并行，没有什么可讲的，没意思。

/**
 * 
 * 注：抽机会把opencv的gpu部分看看
 * 
 * 算了，说干就干！
 */ 
OPENCV - GPU Module Introduction
	http://opencv.org/platforms/cuda.html


...TMD...opencv都出3.0了！新特性官方解释：
	http://code.opencv.org/projects/opencv/wiki/ChangeLog
	OpenCV 3.0 brings more GPU accelerated functions and makes it in much more convenient form than OpenCV 2.4.
	
	The new technology is nick-named "Transparent API" and, in brief, is extension of classical OpenCV functions, such as cv::resize(), to use OpenCL underneath. See more details about here: T-API.
	
	Along with OpenCL code refactoring and Transparent API implementation OpenCL kernels were optimized for mainstream platforms, most notably for modern Intel chips (including Iris and Iris Pro) and AMD chips (such as Kaveri). More detailed results are to be provided later.
















































































































































































































































































































































