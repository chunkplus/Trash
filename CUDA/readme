/**
 * 
 * About
 * 
 * 在cuda外围打转很长时间，关于架构的知识不打算再纠结下去，从今天开始切入代码学习。
 * 主要方法是通过重写cuda samples 的同时google代码解析，同时学习架构。
 * 
 * chunk 2014
 * 
**/


//0902

今天第一个例程是asyncAPI，网上没有找到解析。
不过程序readme中给出：This sample uses CUDA streams and events to overlap execution on CPU and GPU.
// This sample illustrates the usage of CUDA events for both GPU timing and
// overlapping CPU and GPU execution.  Events are insterted into a stream
// of CUDA calls.  Since CUDA stream calls are asynchronous, the CPU can
// perform computations while GPU is executing (including DMA memcopies
// between the host and device).  CPU can query CUDA events to determine
// whether GPU has completed tasks.


这个程序很小，只有一个.cu文件。作为第一个例程，makefile和程序的结构是重点。

0.makefile
	$^ 所有的依赖目标的集合。以空格分隔。如果在依赖目标中有多个重复的,那个这个变量会去除重复的依赖目标,只保留一份。
	$@   表示规则中的目标文件集。在模式规则中,如果有多个目标,那么,"$@"就是匹配于目标中模式定义的集合。

	= 是最基本的赋值
	:= 是覆盖之前的值
	?= 是如果没有被赋值过就赋予等号后面的值
	+= 是添加等号后面的值

1.INCLUDES  := -I../../common/inc
2.cuda编程模型
《cuda编程模型》
	记住三个图：
	1.三级线程组织
	2.内存分配模型
	3.硬件(SM和内存)结构
	
	其中，内存分配模型是pull request，当block分配至SM之后，由硬件对request进行检查、分配。
3.cuda编译
(1)浅析CUDA编译流程与配置方法 (我们现在需要的是cuda程序的编译结构(例如.cu和cpp文件的综合处理))
	http://blog.csdn.net/shi06/article/details/5110017
	Nvcc是一种编译器驱动，通过命令行选项可以在不同阶段启动不同的工具完成编译工作，其目的在于隐藏了复杂的CUDA编译细节，并且它不是一个特殊的CUDA编译驱动而是在模仿一般的通用编译驱动如gcc，它接受一定的传统编译选项如宏定义，库函数路径以及编译过程控制等。CUDA程序编译的路径会因在编译选项时设置的不同CUDA运行模式而不同，如模拟环境的设置等。nvcc封装了四种内部编译工具，即在C:/CUDA/bin目录下的nvopencc(C:/CUDA/open64/bin)，ptxas，fatbin，cudafe

	1.首先是对输入的cu文件有一个预处理过程
		cudafe被称为CUDA frontend，会被调用两次，完成两个工作：一是将主机代码与设备代码分离，生成gpu文件，二是对gpu文件进行dead code analysis，传给nvopencc。 Nvopencc生成ptx文件传给ptxas，最后将cubin或ptx传给fatbin。
	2.同时，在编译阶段CUDA源代码对C语言所扩展的部分将被转成regular ANSI C的源文件，也就可以由一般的C编译器进行更多的编译和连接。
		也即是设备代码被编译成ptx（parallel thread execution）代码或二进制代码，host代码则以C文件形式输出，在编译时可将设备代码链接到所生成的host代码，将其中的cubin对象作为全局初始化数据数组包含进来
	Nvcc的各个编译阶段以及行为是可以通过组合输入文件名和选项命令进行选择的。它是不区分输入文件类型的，如object, library or resource files，仅仅把当前要执行编译阶段需要的文件传递给linker。

	
(2)参考 NVIDIA CUDA Compiler Driver NVCC 官方文档 - http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#axzz3CA54wmUH

	This compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file, and several of these steps are subtly different for different modes of CUDA compilation (such as compilation for device emulation, or the generation of device code repositories). It is the purpose of the CUDA compiler driver nvcc to hide the intricate details of CUDA compilation from developers.

	Compilation Phases
(3)Another Example ：

	http://stackoverflow.com/questions/9421108/g-nvcc-how-to-compile-cuda-code-then-link-it-to-a-g-c-project/9505239#9505239

		all: program
		program: cudacode.o
			g++ -o program -L/usr/local/cuda/lib64 -lcuda -lcudart main.cpp  cudacode.o 
		cudacode.o:
			nvcc -c -arch=sm_20 cudacode.cu 
		clean: rm -rf *o program
	
	暴力方式：
	
		all:
			nvcc cudafile.cu mainfile.cpp -o executable
		clean:
			rm -rf *.o
			
(4)(from http://stackoverflow.com/users/749748/harrism)
	A couple of additional notes:

	1.You don't need to compile your .cu to a .cubin or .ptx file. You need to compile it to a .o object file and then link it with the .o object files from your .cpp files compiled wiht g++.
	
	2.In addition to putting your cuda kernel code in cudaFunc.cu, you also need to put a C or C++ wrapper function in that file that launches the kernel (unless you are using the CUDA driver API, which is unlikely and not recommended). Also add a header file with the prototype of this wrapper function so that you can include it in your C++ code which needs to call the CUDA code. Then you link the files together using your standard g++ link line.

	http://stackoverflow.com/questions/9363827/building-gpl-c-program-with-cuda-module

(5)/**
看起来像是终级解决方案：
	http://stackoverflow.com/questions/9363827/building-gpl-c-program-with-cuda-module
	
	You don't need to compile everything with nvcc. Your guess that you can just compile your CUDA code with NVCC and leave everything else (except linking) is correct. Here's the approach I would use to start.

	1. Add a 1 new header (e.g. myCudaImplementation.h) and 1 new source file (with .cu extension, e.g. myCudaImplementation.cpp). The source file contains your kernel implementation as well as a (host) C wrapper function that invokes the kernel with the appropriate execution configuration (aka <<<>>>) and arguments. The header file contains the prototype for the C wrapper function. Let's call that wrapper function runCudaImplementation()

	2. I would also provide another host C function in the source file (with prototype in the header) that queries and configures the GPU devices present and returns true if it is successful, false if not. Let's call this function configureCudaDevice().

	3.Now in your original C code, where you would normally call your CPU implementation you can do this.
		// must include your new header
		#include "myCudaImplementation.h"

		// at app initialization
		// store this variable somewhere you can access it later
		bool deviceConfigured = configureCudaDevice;          
		...                             
		// then later, at run time
		if (deviceConfigured) 
			runCudaImplementation();
		else
			runCpuImplementation(); // run the original code
	
	4.(最重要的)Now, since you put all your CUDA code in a new .cu file, you only have to compile that file with nvcc. Everything else stays the same, except that you have to link in the object file that nvcc outputs. e.g.
 
		nvcc -c -o myCudaImplementation.o myCudaImplementation.cu <other necessary arguments>

	Then add myCudaImplementation.o to your link line (something like:) g++ -o myApp myCudaImplementation.o

	5.(补充)Got it working. One additional step others may need to do is add extern "C" { } around the wrapper method. This is probably obvious to linking veterans, though. –  emulcahy
	(再补充)You shouldn't have to use extern "C" unless you only have C linkage (e.g. calling it from a .c file). –  harrism
*/
(6)	补充：
	关于extern "C" {} http://xlhnuaa.blog.163.com/blog/static/17233660320124293147308/
	1. 使用extern和包含头文件来引用函数有什么区别呢？extern的引用方式比包含头文件要简洁得多！extern的使用方法是直接了当的，想引用哪个函数就用extern声明哪个函数。这大概是KISS原则的一种体现吧！这样做的一个明显的好处是，会加速程序的编译（确切的说是预处理）的过程，节省时间。在大型C程序编译过程中，这种差异是非常明显的。
	
	2. 　作为一种面向对象的语言，C++支持函数重载，而过程式语言C则不支持。函数被C++编译后在符号库中的名字与C语言的不同。例如，假设某个函数的原型为：voidfoo( int x, int y );该函数被C编译器编译后在符号库中的名字为_foo，而C++编译器则会产生像_foo_int_int之类的名字
	
	未加extern "C"声明时的连接方式 - 实际上，在连接阶段，连接器会从模块A生成的目标文件moduleA.obj中寻找_foo_int_int这样的符号！
	
	加extern "C"声明后的编译和连接方式 - 
	extern "C" int foo( int x, int y );
	在模块B的实现文件中仍然调用foo（2,3），其结果是：（1）模块A编译生成foo的目标代码时，没有对其名字进行特殊处理，采用了C语言的方式；（2）连接器在为模块B的目标代码寻找foo(2,3)调用时，寻找的是未经修改的符号名_foo。

	3.extern "C"通常的使用技巧

(7) 关于架构选项
	http://docs.nvidia.com/cuda/maxwell-compatibility-guide/#verifying-maxwell-compatibility-using-cuda-6-0

	Note that compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.

	The NVIDIA CUDA C compiler, nvcc, can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number. For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.0 (Maxwell) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.

	When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU's native cubin format before launching it. If neither is available, then the kernel launch will fail.



	注意：检测到capabilitie = 2.1 情况下，在eclispe中无论自己指定gencode如何，最终它都会自动添加-gencode arch=compute_20,code=sm_21选项。即，对于capabilitie = 2.1，今后我们编译时只需要指定-gencode arch=compute_20,code=sm_21 .
	如果只指定gpu不指定ptx，则 同样会有-gencode arch=compute_20,code=sm_21
	但是不能都不指定，否则会 -gencode arch=compute_10,code=sm_10
	

4.代码分析
	0.kernel <<<>>>
	核函数只能在主机端调用，调用时必须申明执行参数。调用形式如下：
		Kernel<<<Dg,Db, Ns, S>>>(param list);
			<<<>>>运算符对kernel函数完整的执行配置参数形式是<<<Dg, Db, Ns, S>>>

			Dg：用于定义整个grid的维度和尺寸，即一个grid有多少个block。为dim3类型。Dim3 Dg(Dg.x, Dg.y, 1)表示grid中每行有Dg.x个block，每列有Dg.y个block，第三维恒为1(目前一个核函数只有一个grid)。整个grid中共有Dg.x*Dg.y个block，其中Dg.x和Dg.y最大值为65535。
			
			Db：用于定义一个block的维度和尺寸，即一个block有多少个thread。为dim3类型。Dim3 Db(Db.x, Db.y, Db.z)表示整个block中每行有Db.x个thread，每列有Db.y个thread，高度为Db.z。Db.x和Db.y最大值为512，Db.z最大值为62。 一个block中共有Db.x*Db.y*Db.z个thread。计算能力为1.0,1.1的硬件该乘积的最大值为768，计算能力为1.2,1.3的硬件支持的最大值为1024。
			
			Ns：是一个可选参数，用于设置每个block除了静态分配的shared Memory以外，最多能动态分配的shared memory大小，单位为byte。不需要动态分配时该值为0或省略不写。
			
			S：是一个cudaStream_t类型的可选参数，初始值为零，表示该核函数处在哪个流之中。


	1. cudaError_t CUDARTAPI cudaMallocHost(void **ptr, size_t size);
	关于使用了cudaMallocHost之后发生 segment fault的原因
	由于cudaMallocHost或者其完全版本 cudaHostAlloc，他们都是在host side申请空间，所以所得到的指针都是属于主机端的，但是这个与使用标准库函数malloc／alloc等略有不同，因为使用cuda＊申请的空间的指针是由cuda来维护的，一般的，这里发生segment fault，一般都是由于相应指针定义失效，以至于相应的引用指向无效位置，而这种使之失活的操作（在我今天的错误当中）就是 cudaDeviceReset(void)；在cudaDeviceReset(void)之后出现的对于之前cudaMallocHost的指针，如果再引用，会导致无效引用，错误产生；

	2.struct __device_builtin__ cudaDeviceProp

	3.cudaGetDeviceProperties(&deviceProps, devID)

	4.定义于inc/helper_cuda.h:
		#define checkCudaErrors(val)           check ( (val), #val, __FILE__, __LINE__ )

	5.cudaEvent_t ： 事件
		cudaEvent_t start,stop;
		cudaEventCreate(&start);
		cudaEventCreate(&stop);
		cudaEventRecord(start,0);
		
		其中，cudaEventRecord是产生事件并记录（用于记录当前场景）

		使用事件来测量性能
        1. 用途：为了测量GPU在某个任务上花费的时间。CUDA中的事件本质上是一个GPU时间戳。由于事件是直接在GPU上实现的。因此不适用于对同时包含设备代码和主机代码的混合代码设计。
        2. 形式：首先创建一个事件，然后记录事件，再计算两个事件之差，最后销毁事件。如：		
			cudaEvent_t start, stop;
			cudaEventCreate( &start );
			cudaEventCreate( &stop );
			cudaEventRecord( start, 0 );
			//do something
			cudaEventRecord( stop, 0 );
			float   elapsedTime;
			cudaEventElapsedTime( &elapsedTime,start, stop );
			cudaEventDestroy( start );
			cudaEventDestroy( stop )；

	6.关于cutil.h 
	http://stackoverflow.com/questions/12474191/cuda5-examples-has-anyone-translated-some-cutil-definitions-to-cuda5
		被helper_*.h代替，如
		helper_cuda.h, helper_cuda_gl.h, helper_cuda_drvapi.h, helper_functions.h, helper_image.h, helper_math.h, helper_string.h, and helper_timer.h
		函数cutil* / cut* 被 sdk*取代。
		如 cutCreateTimer(&timer); 替换成 sdkCreateTimer(&timer); 


	7.cudaMemcpyAsync ： cuda流
	http://www.tuicool.com/articles/fY7ryi
		CUDA流表示一个GPU操作队列，并且该队列中的操作以添加到队列的先后顺序执行。使用CUDA流可以实现任务级的并行，比如当GPU在执行核函数的同时，还可以在主机和设备之间交换数据(前提是GPU支持重叠，property的deviceOverlay为true)。
		cudaMemcpyAsync只是放置一个请求，表示在流中执行一次内存复制操作。函数返回时，复制操作不一定启动或执行结束，只是该操作被放入执行队列，在下一个被放入流中的操作之前执行。
		cuda流的使用能提高程序的执行效率。原理主要是使数据复制操作和核函数执行操作交叉执行，不用等到第一次核函数执行结束再开始第二轮的数据复制，以减少顺序执行带来的延迟(类似于编译中使用流水线在解决冲突的前提下提高效率)。
		(为此我们可能需要有多个流，如streamTest.cu，其中有两行：
			kernel<<<N/256, 256, 0, stream0>>>(dev_a0, dev_b0, dev_c0);
			kernel<<<N/256, 256, 0, stream1>>>(dev_a1, dev_b1, dev_c1);
		)
		

5.users' word
	http://bookc.github.io/
	CUDA流表示一个GPU操作队列，并且该队列中的操作将以指定的顺序执行。我们可以在流中添加一些操作，如
	
		核函数启动，内存复制 以及 事件的启动和结束
		
	等。这些操作的添加到流的顺序也是它们的执行顺序。可以将每个流视为GPU上的一个任务，并且这些任务可以并行执行。

	流同步：通过cudaStreamSynchronize()来协调。


7.其他
	1.关于cudaMallocHost and cudaHostAlloc
	The difference between the two if you're compiling with nvcc: nothing. The only difference is if you're using the runtime API from a C program rather than a C++ program (nvcc always compiles as C++), and then cudaHostAlloc is how you specify flags for your allocation.

	2.关于syncronize （barrier）
		cudaStreamSynchronize vs CudaDeviceSynchronize vs cudaThreadSynchronize
		These are all barriers. Barriers prevent code execution beyond the barrier until some condition is met.

		cudaDeviceSynchronize() halts execution in the CPU/host thread (that the cudaDeviceSynchronize was issued in) until the GPU has finished processing all previously requested cuda tasks (kernels, data copies, etc.)
		
		cudaThreadSynchronize() as you've discovered, is just a deprecated version of cudaDeviceSynchronize. Deprecated just means that it still works for now, but it's recommended not to use it (use cudaDeviceSynchronize instead) and in the future, it may become unsupported. But cudaThreadSynchronize() and cudaDeviceSynchronize() are basically identical.
		
		cudaStreamSynchronize() is similar to the above two functions, but it prevents further execution in the CPU host thread until the GPU has finished processing all previously requested cuda tasks that were issued in the referenced stream. So cudaStreamSynchronize() takes a stream id as it's only parameter. cuda tasks issued in other streams may or may not be complete when CPU code execution continues beyond this barrier.


	3.计时
		
		/**
		 * timer utils
		 */
		#include <sys/time.h>
		static struct timeval _tstart, _tend;
		static struct timezone tz;
		void tstart(void) {
			gettimeofday(&_tstart, &tz);
		}
		void tend(void) {
			gettimeofday(&_tend, &tz);
		}
		double tval() {
			double t1, t2;
			t1 = (double) _tstart.tv_sec + (double) _tstart.tv_usec / (1000 * 1000);
			t2 = (double) _tend.tv_sec + (double) _tend.tv_usec / (1000 * 1000);
			return t2 - t1;
		}

		或者：
		#include <sys/time.h>
		class mTimer {
			struct timeval _tstart, _tend;
			struct timezone tz;
		public:
			void start() {
				gettimeofday(&_tstart, &tz);
			}
			void end() {
				gettimeofday(&_tend, &tz);
			}
			double getTime() {
				double t1, t2;
				t1 = (double) _tstart.tv_sec + (double) _tstart.tv_usec / (1000 * 1000);
				t2 = (double) _tend.tv_sec + (double) _tend.tv_usec / (1000 * 1000);
				return t2 - t1;
			}
			void showTime() {
				printf("time in seconds:\t%.12f s\n", getTime());
			}
		};

		或者：
		
		class cuTimer {
			StopWatchInterface *timer;
		public:
			cuTimer() {
				sdkCreateTimer(&timer);
			}
			~cuTimer() {
				sdkDeleteTimer(&timer);
			}
			void start() {
				sdkStartTimer(&timer);
			}
			void end() {
				sdkStopTimer(&timer);
			}
			double getTime() {
				return sdkGetTimerValue(&timer);
			}
			//Time in msec
			void showTime() {
				printf("time in msec:\t%.12f ms\n", getTime());
			}
		};


	4.关于生命周期！注意！
	除非显式cudaFree或者GPU重启，否则即使程序结束或者cudaDeviceReset也不能将显存中的数据清除。
	
	cudaDeviceReset : Explicitly destroys and cleans up all resources associated with the current device in the current process. Any subsequent API call to this device will reinitialize the device.
	Note that this function will reset the device immediately. It is the caller's responsibility to ensure that the device is not being accessed by any other host threads from the process when this function is called.
	
		其他方法：
		1.nvidia-smi -i 0 -r
			Resetting GPU is not supported on this device.
			Treating as warning and moving on.
			All done.
		
	5.TMD!随机数也有问题！尤其是在eclipse中，如果不设置srand，随机序列时不回变化的！
		惊出一身冷汗，这让我想到了我的密码学作业。。。

8.关于并发(concurrent)与并行(parallel)
	http://www.doc88.com/p-944523615763.html


注意：未解决问题 - reset


//0903

今天第二个例程是clock，因为CDP(cuda dynamic parallism)要求capabilitie 在3.5以上。

2_clock的目的是通过device端的timer计算精确的（不包括IO）的时钟时间，然后将这个时间传回host读取。
本程序涉及几个重要的话题。

1.__shared__
	http://stackoverflow.com/questions/5531247/allocating-shared-memory
	
	(1)
	__shared__ int a[count]; //error
	
	“const doesn't mean "constant", it means "read-only".”
	A constant expression is something whose value is known to the compiler at compile-time.
	(2)
	__shared__ int a[100];
    __shared__ int b[4];	//ok
	
	or
	
	extern __shared__ int *shared;
    int *a = &shared[0]; //a is manually set at the beginning of shared
    int *b = &shared[count_a]; //b is manually set at the end of a

	(3)
	extern __shared__ int a[]; //ok
	...
	Kernel<<< gridDim, blockDim, a_size >>>(count);
	
	CUDA supports dynamic shared memory allocation. If you declare the kernel like that。
	
	也就是说，cuda的动态分配是在kernel参数中体现的，Ds指定了多少，最终就一定会分配这么多，并且顺序供给，用不完也在那。
	
2.至于为什么动态分配时是extern，解释如下：
	(1)本意：生命某个全局变量存在于某个源文件的某处，让编译器放心
	http://stackoverflow.com/questions/10422034/when-to-use-extern-in-c
	extern在c语言中的本意：This comes in useful when you have global variables. You declare the existence of global variables in a header, so that each source file that includes the header knows about it, but you only need to “define” it once in one of your source files.
	
	To clarify, using extern int x; tells the compiler that an object of type int called x exists somewhere. It's not the compilers job to know where it exists, it just needs to know the type and name so it knows how to use it. 

	(2) 关于 extern "C"
	http://stackoverflow.com/questions/1041866/in-c-source-what-is-the-effect-of-extern-c
	Since C++ has overloading of function names and C does not, the C++ compiler cannot just use the function name as a unique id to link to, so it mangles the name by adding information about the arguments. A C compiler does not need to mangle the name since you can not overload function names in C. When you state that a function has extern "C" linkage in C++, the C++ compiler does not add argument/parameter type information to the name used for linkage.
	这里的目的不是欺骗编译器，而是指导编译器在编译过程中对这个保护区进行特殊处理。
	一种跨c/c++的编码方式：
		#ifdef __cplusplus
		extern "C" {
		#endif

		// all of your legacy C code here

		#ifdef __cplusplus
		}
		#endif
	
	(2) extern与头文件(*.h)的区别和联系
	http://lpy999.blog.163.com/blog/static/117372061201182051413310/
	http://blog.csdn.net/yuyantai1234/article/details/7245412
	
	首先说下头文件，其实头文件对计算机而言没什么作用，她只是在预编译时在#include的地方展开一下，没别的意义了，其实头文件主要是给别人看的。
	
	那既然是说明，那么头文件里面放的自然就是关于函数，变量，类的“声明”(对函数来说，也叫函数原型)了。记着，是“声明”，不是“定义”。那么，我假设大家知道声明和定义的区别。所以，最好不要傻嘻嘻的在头文件里定义什么东西。比如全局变量.
	
	这个关键字真的比较可恶，在定义变量的时候，这个extern居然可以被省略(定义时，默认均省略)；在声明变量的时候，这个extern必须添加在变量前，所以有时会让你搞不清楚到底是声明还是定义。或者说，变量前有extern不一定就是声明，而变量前无extern就只能是定义。注：定义要为变量分配内存空间；而声明不需要为变量分配内存空间。
	
	  函数，对于函数也一样，也是定义和声明，定义的时候用extern，说明这个函数是可以被外部引用的，声明的时候用extern说明这是一个声明。 但由于函数的定义和声明是有区别的，定义函数要有函数体，声明函数没有函数体(还有以分号结尾)，所以函数定义和声明时都可以将extern省略掉，反正其他文件也是知道这个函数是在其他地方定义的，所以不加extern也行。
	  
	/***
	 * 注意：通过上述可知，对于函数来说，关键字'extern'可有可无；对于变量来说，声明时必须有extern。
	 * 为统一起见，我们将全局函数和变量的声明都加上extern。
	 * 注意，对于变量来说，如果需要声明的时候同时定义（在同一source文件中），则直接定义，不加extern，这与我们原先的习惯不矛盾。
	 */
	
	发现了没有？从此可以省略.h文件了！extern可以看成精简版的胶水。
	
	
	
	同时需要注意：we should also mention that this means that if you have two kernels with shared memory, you should use different identifiers for the shared memory pointer. it's not completely necessary but can help avoid some errors.nvcc wouldn't compile when i had extern __shared__ int *shared in one kernel and extern __shared__ float *shared in another kernel 
	

3.同步函数__syncthreads()
	__syncthreads()是 CUDA 的内置命令，其作用是保证 block 内的所有线程都已经运行到调用__syncthreads()的位置，这样可以保证各个线程看到的存储器是一样的。其头文件为 device_functions.h
	
	http://www.cnblogs.com/dwdxdy/p/3215136.html
	一句话：块内线程同步
	
	
	
4.规约法

	参考官方文档 - 关于cuda规约优化 http://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
		by Mark Harris (http://www.markmark.net/ , http://stackoverflow.com/users/749748/harrism)

	部分翻译 ： http://hackecho.com/2013/04/cuda-parallel-reduction/

5.其他
	1.rand()
	h_A[i] = (int) ((float) NUM_THREADS * rand() / (RAND_MAX + 1.0));
	其中(float) 不能落下，否则全零。
	
	2.checkCudaErrors
	checkCudaErrors还是很有用的，比如用printf调试了半天无果，加上checkCudaErrors一检验，立马报错：CUDA error at ../src/clock.cu:76 code=77(<unknown>) "cudaMemcpy(h_timer, d_timer, NUM_BLOCKS * 2 * sizeof(clock_t), cudaMemcpyDeviceToHost)" 

	关于错误检查！！！！！坑爹！
	http://stackoverflow.com/questions/19172408/acudaerrorunknown-in-cudamemcpy-function-call
	The documentation for both cudaMemcpy and cudaFree contains the following note:

	Note that this function may also return error codes from previous, asynchronous launches.

	ie. the error isn't happening in either cudaMemcpy or cudaFree, rather it is happening during the previous kernel launch or execution. If you follow this advice and modify your code to something like this:

		vectorMPL<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_D, numElements);
		checkCudaErrors(cudaPeekAtLastError());
		checkCudaErrors(cudaDeviceSynchronize());
		
	You should find that the error is reported by the cudaDeviceSynchronize() call, indicating that the error occurred when kernel was executing. The underlying reason for the error will most likely be out of bounds memory access ... ...
	
	3.bug eye for this code.
	timer[bid + blockDim.x] = clock();	// HERE : bug eye!

6.还有个重要的问题未解决：测试最佳blocksize

/**
 * 2015-01
 * 
补充：calculate mem bandwith
http://www.enthusiastpc.net/articles/00001/3.aspx

Memory bandwidth. 

Okay now that we have touched on the subjects of memory clockspeed and memory bus width we can get to the most important part: memory bandwidth. Memory bandwidth tells us the speed at which the GPU can access the memory. Remember with GPU processing power how this was a combination of the number of cores and the clockspeed? Well, something similar is the case with the memory. How much memory can be accessed per second is a combination of the bus width and the memory datarate. This is why clockspeed by itself does not tell you anything about the real memory speed available to the GPU. 

Calculating the memory bandwidth is quite simple. Consider the following example for the reference GTX 580 card from NVidia:

Item	Value
Memory clock	1002MHz
Memory bus width	384-bit

A memory clock of 1002MHz means a datarate of 4008MHz. Put more simply this means that the memory can do 4008 million transactions per second. Each of these transactions is 384 bits. This means that the bandwidth of the memory is 384 x 4008 = 1,539,072 million bits per second. To get from bits to bytes we divide the number by 8 leading to 1,539,072 / 8 = 192,384 million bytes per second or 192.4 GB/s. This final outcome is the real memory performance figure :-) 
*/

800*2*64 = 102.4Gbps = 12.8GB/s (Device to Device, or D2D)

	
事先一定要把结构图画清楚！！

//0904

今天第三个例程是cppIntegration，这也是阴影之一，力图破之。

readme很清楚: This example demonstrates how to integrate CUDA into an existing C++ application, i.e. the CUDA entry point on host side is only a function which is called from C++ code and only the file containing this function is compiled with nvcc. It also demonstrates that vector types can be used from cpp.

1.extern
	这个问题之前已经讨论过了。
2.关于bank conflicts
	// use integer arithmetic to process all four bytes with one thread
	// this serializes the execution, but is the simplest solutions to avoid
	// bank conflicts for this very low number of threads
	// in general it is more efficient to process each byte by a separate thread,
	// to avoid bank conflicts the access pattern should be
	// g_data[4 * wtid + wid], where wtid is the thread id within the half warp
	// and wid is the warp id
	// see also the programming guide for a more in depth discussion.
	看完 https://www.youtube.com/watch?v=CZgM3DEBplE 
	就明白了。
	首先，访存是按对齐的字的方式，因此以int方式会大大提高效率
	
	https://www.youtube.com/watch?v=CZgM3DEBplE  中同时给出了测试bank conflict的程序，过会儿可以试一下。
	
3.用int代char的方式
	这样进行批量处理有哪些优势和劣势？


//0904

今天第四个例程是cppOverload. readme中解释 ： This sample demonstrates how to use C++ function overloading on the GPU.

1.除法向上取整固定套路：
	#define DIV_UP(a, b) (((a) + (b) - 1) / (b))
	用在计算blocksize（即griddim）时 ： <<<DIV_UP(N, THREAD_N), THREAD_N>>>
2.再次讨论cudaDeviceReset
	// cudaDeviceReset causes the driver to clean up all state. While
	// not mandatory in normal operation, it is good practice.  It is also
	// needed to ensure correct operation when the application is being
	// profiled. Calling cudaDeviceReset causes all profile data to be
	// flushed before the application exits

3.shared_mem 和 L1_cache的分配
	cudaFuncSetCacheConfig(*func2, cudaFuncCachePreferShared)；


//0905

从今天开始，我们要简略的过几个0_sample中的samples，需要加快些脚步。。。

/**
 * cudaOpenMP
 */
	This sample demonstrates how to use OpenMP API to write an application for multiple GPUs.  This executable is not pre-built with the SDK installer.
1.-lgomp
	不知道为什么以前没有链接这个库没事，现在除了编译指定-fopenmp外，还必须链接这个库。
	位置/usr/lib/gcc/x86_64-linux-gnu/4.6/libgomp.so
	http://gcc.gnu.org/projects/gomp/ 
		GOMP — An OpenMP implementation for GCC - GNU Project

2.本程序实现的是多device(GPU)之间通过openmp并行，没有什么可讲的，没意思。

/**
 * 
 * 注：抽机会把opencv的gpu部分看看
 * 
 * 算了，说干就干！
 */ 
OPENCV - GPU Module Introduction
	http://opencv.org/platforms/cuda.html


...TMD...opencv都出3.0了！新特性官方解释：
	http://code.opencv.org/projects/opencv/wiki/ChangeLog
	OpenCV 3.0 brings more GPU accelerated functions and makes it in much more convenient form than OpenCV 2.4.
	
	The new technology is nick-named "Transparent API" and, in brief, is extension of classical OpenCV functions, such as cv::resize(), to use OpenCL underneath. See more details about here: T-API.
	
	Along with OpenCL code refactoring and Transparent API implementation OpenCL kernels were optimized for mainstream platforms, most notably for modern Intel chips (including Iris and Iris Pro) and AMD chips (such as Kaveri). More detailed results are to be provided later.

/**
 * 
 * 怂了。。。本着小码怡情的方针，我们还是继续研究基础代码吧。
 * 
 * inlinePTX
 * 
 */ 
A simple test application that demonstrates a new CUDA 4.0 ability to embed PTX in a CUDA kernel.

1.嵌入PTX代码
	asm("mov.u32 %0, %%laneid;" : "=r"(laneid));
	//This command gets the lane ID within the current warp

	参考官方文档 - http://docs.nvidia.com/cuda/parallel-thread-execution/index.html#axzz3Cd8Fah6X
	
	PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).

	A key difference is that SIMD vector organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread.

	SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads. For the purposes of correctness, the programmer can essentially ignore the SIMT behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. In practice, this is analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance. Vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually.

CUDA Pro Tip: View Assembly Code Correlation in Nsight Visual Studio Edition
	http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-view-assembly-code-correlation-nsight-visual-studio-edition/
	As Mark Harris explained in the previous CUDA Pro Tip, there are two compilation stages required before a kernel written in CUDA C can be executed on the GPU. The first stage compiles the high-level C code into the PTX virtual GPU ISA. The second stage compiles PTX into the actual ISA of the hardware, called SASS (details of SASS can be found in the cuobjdump.pdf installed in the doc folder of the CUDA Toolkit). 

	The hardware ISA is in general different between GPU architectures. To allow forward compatibility, the second compilation phase can be either done as part of the normal compilation using nvcc or at runtime using the integrated JIT compiler in the driver.

2.今天一个重要的话题是 cuda的调试
Remote application development using NVIDIA® Nsight™ Eclipse Edition（关于cuda调试和profile的）
	http://devblogs.nvidia.com/parallelforall/remote-application-development-nvidia-nsight-eclipse-edition/

	Profiling Your Remote Application

	We need to create a release build before we launch the profiler. You can enable the -lineinfo option in the compile options to generate information on source-to-SASS instruction correlation. To do this, first go to the project settings by right-clicking on the project in the left pane. Then navigate to Properties->Build->Settings->Tool Settings->Debugging and check the box that says “Generate line-number…” and click Apply. 

	Back in the main window, click on the build hammer drop-down menu to create a release build. Once the build is ready, profile your remote application by clicking on the “Profile” icon on the right of the Run icon. In the drop-down menu select “Profile AS->Remote C/C++ Application”, Nsight will prompt you to select the binaries; choose the release binary so it runs on the target system.

	Let’s select the collideD kernel and then click on the “unguided analysis” icon under the “Analysis” tab. Then scroll down to click on “Kernel Profile”, which will analyze the source code of the collideD kernel and map it to the executed instructions. Once the analysis finishes, the pane on the right shows the kernel name. Double click on collideD and Nsight will bring up the source-to-SASS assembly instructions view which shows all the hot spots at instruction level. The kernel profile, shown in , as shown in Figure 8, provides the execution count, inactive threads, and predicated threads for each source and assembly line of the kernel. Using this information you can pinpoint portions of your kernel that inefficiently use compute resources due to divergence or predication.


	http://docs.nvidia.com/cuda/profiler-users-guide/#axzz3Cd8Fah6X
	
	太牛逼了！


/**
 * 
 * matrixMul
 * 
 * This sample implements matrix multiplication and is exactly the same as Chapter 6 of the p
 * 
 */ 

readme指出 - It has been written for clarity of exposition to illustrate various CUDA programming principles, not with the goal of providing the most performant generic kernel for matrix multiplication.

1.今晚上又重温了一下矩阵乘优化的问题。三维演示模型见res/*.jpg */
由此引出了几个问题:
	1.cache block 的原理
	2.如何在cuda(或者说GPU)上优化矩阵乘
	3.比较OpenBLAS，Intel MKL和Eigen的矩阵相乘性能
	
	链接 - 
	C++矩阵处理工具——Eigen http://blog.csdn.net/abcjennifer/article/details/7781936
	比较OpenBLAS，Intel MKL和Eigen的矩阵相乘性能 http://www.leexiang.com/the-performance-of-matrix-multiplication-among-openblas-intel-mkl-and-eigen
	Poor performance of OpenBLAS matrix-vector multiplication on small sizes #3239  https://github.com/JuliaLang/julia/issues/3239
	The GPU on the Matrix-Matrix Multiply file:///home/chunk/Desktop/parco09.pdf

2.上述三维模型应用于matrixMul,分析见*.jpg
	注意，这里线程分配是面向最终的结果矩阵C
	注意 __syncthreads();

//0910

/**
 * 
 * matrixMulCUBLAS
 * 
 */ 

cuBLAS的使用是本节的主题。关于BLAS的level1和level2的知识前面提到了。

玩玩CUBLAS(1)——hello cublas - http://blog.csdn.net/bendanban/article/details/8891274
玩玩CUBLAS(2)——level1函数 - http://blog.csdn.net/bendanban/article/details/8893944
玩玩CUBLAS(3)——level2函数 - http://blog.csdn.net/bendanban/article/details/8897735/

	cublas接受的参数，按照存储地点分，有三种：设备上，主机上，设备或者主机上。按照数学类型分：标量（对应单变量，常量），向量（对应数组），矩阵（对应数组）。
	cublas中向量，矩阵从1开始索引。这点要注意的是，cublas自己的函数这样实现，我们自己的数据还是按照自己的语言标准来做的，就是说，你用C语言的数组，还是从0开始，但是用cublas库的时候，库本身自己从1开始。

1. #define CUBLAS_ERROR_CHECK(sdata) if(CUBLAS_STATUS_SUCCESS!=sdata){printf("ERROR at:%s:%d\n",__FILE__,__LINE__);exit(-1);}

	#define CUBLAS_ERROR_CHECK(sdata) if(CUBLAS_STATUS_SUCCESS!=sdata)\  
           {printf("ERROR at:%s:%d\n",__FILE__,__LINE__);exit(-1);} 
2. vec[i] = (float)(rand()%(up-down+1)+down);

3. cublasSetVector(data_size, sizeof(float), h_datax, 1, d_datax, 1);

4.about nvidia
	GT GTX T代表Tesla 40nm制程 2011年
	GK K代表Kepler 28nm制程 2012年之后
	（？是这样的吗？）
	详见*.jpg
	
	Tesla 2007
	Fermi 2009
	Kepler 2011
	Maxwell 2013
5.BLAS中level2的函数包含了矩阵与向量的操作。

	#define IDX2C(i,j,ld) (((j)*(ld))+(i))

	C语言是按行存储的，所以，在内存中，A的首行首先被连续存储，然后是首行后的一行再被连续的存储，依次类推，我们称A的这个存储结构为Host_A。

	那么CUBLAS会如何理解HOST_A呢？

	CUBLAS中，矩阵会被按照列存储。
	（包括下标从1开始，这些特征更靠近matlab）

	我们可以在用的时候在CUBLAS中转置后再用。

6.Blas 基本函数功能
	http://blog.csdn.net/q673327335/article/details/8547576
	
	CAXPY
	constant times a vector plus a vector. //常数乘以一个向量加上一个向量。

	CCOPY
	 copies a vector x to a vector y.//复制一个向量x到y的向量。

	CDOTC
	forms the dot product of two vectors, conjugating the first vector.//结合第一个向量,形成了两个向量的点积

	CDOTU
	forms the dot product of two vectors.//形成了两个向量的点积。

	CGBMV 
	 performs one of the matrix-vector operations

	*  CGEMM  performs one of the matrix-matrix operations
	*
	*     C := alpha*op( A )*op( B ) + beta*C,
	*
	*  where  op( X ) is one of
	*
	*     op( X ) = X   or   op( X ) = X**T   or   op( X ) = X**H,
	*
	*  alpha and beta are scalars, and A, B and C are matrices, with op( A )
	*  an m by k matrix,  op( B )  a  k by n matrix and  C an m by n matrix.

	CGEMM
	performs one of the matrix-matrix operations
	*
	* C := alpha*op( A )*op( B ) + beta*C,
	*
	* where op( X ) is one of
	*
	* op( X ) = X or op( X ) = X**T or op( X ) = X**H,
	*
	* alpha and beta are scalars, and A, B and C are matrices, with op( A )
	* an m by k matrix, op( B ) a k by n matrix and C an m by n matrix.
	*

C := alpha * op( A ) * op( B ) + beta * C
	SUBROUTINE SGEMM(TRANSA, TRANSB, M, N, K,
	ALPHA, A, LDA, B, LDB, BETA, C, LDC)
	CHARACTER*1  TRANSA, TRANSB
	INTEGER  M, N, K, LDA, LDB, LDC
	REAL  ALPHA, BETA
	REAL  A(LDA,*),  B(LDB,*),  C(LDC,*)

	SUBROUTINE DGEMM(TRANSA, TRANSB, M, N, K,
	ALPHA, A, LDA, B, LDB, BETA, C, LDC)
	CHARACTER*1 TRANSA,TRANSB
	INTEGER M,N,K,LDA,LDB,LDC
	DOUBLE PRECISION ALPHA,BETA
	DOUBLE PRECISION A(LDA,*), B(LDB,*), C(LDC,*)
	
	SUBROUTINE CGEMM(TRANSA, TRANSB, M, N, K,
	ALPHA, A, LDA, B, LDB, BETA, C, LDC)
	CHARACTER*1 TRANSA,TRANSB
	INTEGER M,N,K,LDA,LDB,LDC
	COMPLEX ALPHA,BETA
	COMPLEX A(LDA,*), B(LDB,*), C(LDC,*)
	
	SUBROUTINE ZGEMM(TRANSA, TRANSB, M, N, K,
	ALPHA, A, LDA, B, LDB, BETA, C, LDC)
	CHARACTER*1 TRANSA,TRANSB
	INTEGER M,N,K,LDA,LDB,LDC
	COMPLEX*16 ALPHA,BETA
	COMPLEX*16 A(LDA,*), B(LDB,*), C(LDC,*)


/**
 * 
 * matrixMulDrv
 * 
 * This sample implements matrix multiplication and is exactly the same as Chapter 6 of the p
 * 
 */
1.#pragma unroll
#pragma unroll
编译器默认情况下将循环展开小的次数，#pragma unroll 能够指定循环以多少次展开（程序员必须保证按这个展开是正确的），例如

#pragma unroll 5
for()

pragma unroll 后必须紧接着处理的循环。

#pragmatic unroll 1 禁止编译器将循环展开。

如果没指定次数，对于常数次的循环，循环将完全展开，对于不确定次数的循环，循环将不会被展开。

2.执行kernel的第二种方式 
	cuLaunchKernel(matrixMul, grid.x, grid.y, grid.z, block.x, block.y, block.z, 2*block_size*block_size*sizeof(float), NULL, args, NULL));

其实本例子的主要意图在于ptx和cpp混合编程，可以看做是嵌入汇编。



	/**
	 * \brief Load a module's data with options
	 *
	 * Takes a pointer \p image and loads the corresponding module \p module into
	 * the current context. The pointer may be obtained by mapping a \e cubin or
	 * \e PTX or \e fatbin file, passing a \e cubin or \e PTX or \e fatbin file
	 * as a NULL-terminated text string, or incorporating a \e cubin or \e fatbin
	 * object into the executable resources and using operating system calls such
	 * as Windows \c FindResource() to obtain the pointer. Options are passed as
	 * an array via \p options and any corresponding parameters are passed in
	 * \p optionValues. The number of total options is supplied via \p numOptions.
	 * Any outputs will be returned via \p optionValues. 
	 *
	 * \param module       - Returned module
	 * \param image        - Module data to load
	 * \param numOptions   - Number of options
	 * \param options      - Options for JIT
	 * \param optionValues - Option values for JIT
	 */
	CUresult CUDAAPI cuModuleLoadDataEx(CUmodule *module, const void *image, unsigned int numOptions, CUjit_option *options, void **optionValues);

	/**
	 * \brief Returns a function handle
	 *
	 * Returns in \p *hfunc the handle of the function of name \p name located in
	 * module \p hmod. If no function of that name exists, ::cuModuleGetFunction()
	 * returns ::CUDA_ERROR_NOT_FOUND.
	 *
	 * \param hfunc - Returned function handle
	 * \param hmod  - Module to retrieve function from
	 * \param name  - Name of function to retrieve
	 */
	CUresult CUDAAPI cuModuleGetFunction(CUfunction *hfunc, CUmodule hmod, const char *name);


3.Drv编译
本例的流程是，首先
注意前几天将部分库(主要的nvidia库)升级到340.29，不知道有没有影响，不过在编译时提醒“WARNING - libcuda.so not found, CUDA Driver is not installed.  Please re-install the driver. ”，后来发现原因在于libcuda.so的路径没有设置好。makefile中添加搜索路径CUDA_SEARCH_PATH += /usr/lib/x86_64-linux-gnu 即可。或者sudo ln -s /usr/lib/x86_64-linux-gnu/libcuda.so /usr/lib/libcuda.so

看一看makefile中的编译过程：
[@] nvcc -ccbin g++ -I../../common/inc -m64 -o matrixMulDrv.o -c matrixMulDrv.cpp
[@] nvcc -ccbin g++ -m64 -o matrixMulDrv matrixMulDrv.o -lcuda
[@] nvcc -ccbin g++ -I../../common/inc -m64 -o matrixMul_kernel64.ptx -ptx matrixMul_kernel.cu


看一看eclipse中的编译过程：
nvcc -I/usr/local/cuda/include/inc -G -g -O3 -gencode arch=compute_20,code=sm_20 -gencode arch=compute_20,code=sm_21  -odir "src" -M -o "src/matrixMulDrv.d" "../src/matrixMulDrv.cpp"
nvcc -I/usr/local/cuda/include/inc -G -g -O3 --compile  -x c++ -o  "src/matrixMulDrv.o" "../src/matrixMulDrv.cpp"
nvcc --cudart static -link -o  "A_someSamples"  ./src/matrixMulDrv.o ./src/matrixMul_kernel.o   -lgomp -lcuda -lcublas	


注释
--ptx   (-ptx)                            
        Compile all .cu/.gpu input files to device- only .ptx files. This step 
        discards the host code for each of these input file.
--compile                                          (-c)                         
        Compile each .c/.cc/.cpp/.cxx/.cu input file into an object file.

--x     (-x)                              
        Explicitly specify the language for the input files, rather than 
        letting the compiler choose a default based on the file name suffix.
        Allowed values for this option:  'c','c++','cu'.
        
--generate-dependencies                            (-M)                         
        Generate for the one .c/.cc/.cpp/.cxx/.cu input file (more than one 
        input file is not allowed in this mode) a dependency file that can be 
        included in a make file.
        
--device-debug                                     (-G)                         
        Generate debug information for device code.
        
--machine <bits>                                   (-m)                         
        Specify 32 vs 64 bit architecture.
        Allowed values for this option:  32,64.
        Default value:  64.

--compiler-bindir <path>                           (-ccbin)                     
		Specify the directory in which the compiler executable (Microsoft 
		Visual Studio cl, or a gcc derivative) resides. By default, this 
		executable is expected in the current executable search path. For a 
		different compiler, or to specify these compilers with a different 
		executable name, specify the path to the compiler including the 
		executable name.

//0914
接下来时一大批的simple*程序，作为cuda各种基础功能的展示。

/**
 * 
 * simpleAtomicIntrinsics
 * 
 * A simple program demonstrating trivial use of global memory atomic
 * device functions (atomic*() functions).
 * 
 */
 
 /**
  * 
补充：__device__ __host__ recursion
https://devtalk.nvidia.com/default/topic/519806/what-is-__host__-__device__/
https://code.google.com/p/stanford-cs193g-sp2010/wiki/TutorialDeviceFunctions
http://stackoverflow.com/questions/19013156/how-does-cuda-4-0-support-recursion

Sometimes the same functionality is needed in both the host and the device portions of CUDA code. To avoid code duplication, CUDA allows such functions to carry both __host__ and __device__ attributes, which means the compiler places one copy of that function into the host compilation flow (to be compiled by the host compiler, e.g. gcc or MSVC), and a second copy into the device compilation flow (to be compiled with NVIDIA's CUDA compiler).

Global functions are also called "kernels" - it's the functions that you may call from the host side using CUDA kernel call semantics (<<<...>>>)

Device functions can only be called from other device or global functions. __device__ functions cannot be called from host code.
* 
* https://devtalk.nvidia.com/default/topic/452958/combined-__host__-__device__-functions-how-to-tell-if-it-is-device-or-host-executing-/
* #ifdef  __CUDA_ARCH__

   arr[threadIdx.x] = arr[threadIdx.x + 1];

#else
* 
* 
*/


1.cuda 原子操作
http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions

	Atomic functions operating on 32-bit integer values in shared memory and atomic functions operating on 64-bit integer values in global memory are only available for devices of compute capability 1.2 and higher.
	Atomic functions operating on 64-bit integer values in shared memory are only available for devices of compute capability 2.x and higher.

[important!!!]
Note however that any atomic operation can be implemented based on atomicCAS() (Compare And Swap). For example, atomicAdd() for double-precision floating-point numbers can be implemented as follows:

	__device__ double atomicAdd(double* address, double val) {
		unsigned long long int* address_as_ull = (unsigned long long int*) address;
		unsigned long long int old = *address_as_ull, assumed;
		do {
			assumed = old;
			old = atomicCAS(address_as_ull, assumed,
					__double_as_longlong(val + __longlong_as_double(assumed)));
			// Note: uses integer comparison to avoid hang in case of NaN (since NaN != NaN) } while (assumed != old); return __longlong_as_double(old); }
		} while (assumed != old);
		return __longlong_as_double(old);
	}


int atomicCAS(int* address, int compare, int val);

	reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old == compare ? val : old) , and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old (Compare And Swap).

	（即 tmp = old;old = (old==compare)?val:old;return tmp; 即同则变，异不变）


http://emuch.net/html/201306/6008188.html


2.复习一下信号量
	
关于并发和并行-
并行(parallel)和并发(concurrent)
	并发和并行是即相似又有区别的两个概念，并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔内发生。在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行，但在单处理机系统中，每一时刻却仅能有一道程序执行，故微观上这些程序只能是分时地交替执行。倘若在计算机系统中有多个处理机，则这些可以并发执行的程序便可被分配到多个处理机上，实现并行执行，即利用每个处理机来处理一个可并发执行的程序，这样，多个程序便可以同时执行。

	信号量(Semaphore)，有时被称为信号灯，是在多线程环境下使用的一种设施，是可以用来保证两个或多个关键代码段不被并发调用。


关于线程安全-
	
	线程安全是编程中的术语，指某个函数 (计算机科学)、函数库在多线程环境中被调用时，能够正确地处理各个线程的局部变量，使程序功能正确完成。

	一般来说，线程安全的函数应该为每个调用它的线程分配专门的空间，来储存需要单独保存的状态（如果需要的话），不依赖于“线程惯性”，把多个线程共享的变量正确对待（如，通知编译器该变量为“易失（volatile）”型，阻止其进行一些不恰当的优化），而且，线程安全的函数一般不应该修改全局对象。

	很多C库代码（比如某些strtok的实现，它将“多次调用中需要保持不变的状态”储存在静态变量中，导致不恰当的共享）不是线程安全的，在多线程环境中调用这些函数时，要进行特别的预防措施，或者寻找别的替代方案。

关于可重入-
	
	若一个程序或子程序可以“安全的被并行执行(Parallel computing)”，则称其为可重入（reentrant或re-entrant）的。即当该子程序正在运行时，可以再次进入并执行它（并行执行时，个别的执行结果，都符合设计时的预期）。可重入概念是在单线程操作系统的时代提出的。一个子程序的重入，可能由于自身原因，如执行了jmp或者call，类似于子程序的递归调用；或者由于硬件中断，UNIX系统的signal的处理，即子程序被中断处理程序或者signal处理程序调用。

可重入与线程安全-
	可重入与线程安全两个概念都关系到函数处理资源的方式。但是，他们有一定的区别。可重入概念会影响函数的外部接口，而线程安全只关心函数的实现。大多数情况下，要将不可重入函数改为可重入的，需要修改函数接口，使得所有的数据都通过函数的调用者提供。要将非线程安全的函数改为线程安全的，则只需要修改函数的实现部分。一般通过加入同步机制以保护共享的资源，使之不会被几个线程同时访问。

	线程安全与可重入性是两个不同性质的概念。可重入是在单线程操作系统背景下，重入的函数或者子程序，按照后进先出的线性序依次执行完毕。多线程执行的函数或子程序，各个线程的执行时机是由操作系统调度，不可预期的，但是该函数的每个执行线程都会不时的获得CPU的时间片，不断向前推进执行进度。可重入函数未必是线程安全的；线程安全函数未必是可重入的。例如，一个函数打开某个文件并读入数据。这个函数是可重入的，因为它的多个实例同时执行不会造成冲突；但它不是线程安全的，因为在它读入文件时可能有别的线程正在修改该文件，为了线程安全必须对文件加“同步锁”。另一个例子，函数在它的函数体内部访问共享资源使用了加锁、解锁操作，所以它是线程安全的，但是却不可重入。因为若该函数一个实例运行到已经执行加锁但未执行解锁时被停下来，系统又启动该函数的另外一个实例，则新的实例在加锁处将转入等待。如果该函数是一个中断处理服务，在中断处理时又发生新的中断将导致资源死锁。


使用c++11中的atomics，可使此函数即线程安全又可重入。

	#include <atomic> 
	int increment_counter ()
	{
		static std::atomic<int> counter(0); 
		// increment is guaranteed to be done atomically
		int result = ++counter; 
		return result;
	}

3.补充 - CUDA shared memory使用
	http://www.hpctech.com/2009/0818/207.html


4.再说 cutil的消失
	http://stackoverflow.com/questions/14919613/where-to-find-cudas-cutil-math-h
	
	可以在 https://developer.nvidia.com/cuda-toolkit-42-archive 下载SDK
	
	The cutil functionality was deleted from the CUDA 5.0 Samples (i.e. the "SDK"). You can still download a previous SDK and compile it under CUDA 5, you should then have everything that came with previous SDK's.

	The official notice was given by nvidia here, in the release notes installation instructions section. As to why, I imagine that the nvidia sentiment regarding cutil probably was something like what is expressed here "not suitable for use in a real application. It is completely unsupported" but people were using it in real applications. So one way to try put a stop to that is to delete it, I suppose. That's just speculation.


	make x86_64=1 SMS="20"

/**
 * 
 * simpleCallback.cu
 * 
 *This sample implements multi-threaded heterogeneous computing workloads with the new CPU callbacks for CUDA *
 * streams and events introduced with CUDA 5.0.
 * Together with the thread safety of the CUDA API implementing heterogenous workloads that float between CPU 
 * threads and GPUs has become simple and efficient.
 *
 * The workloads in the sample follow the form CPU preprocess -> GPU process -> CPU postprocess.
 * Each CPU processing step is handled by its own dedicated thread. GPU workloads are sent to all available GPUs in 
 * the system.
 *
 */
 
 本例程涉及一个重要的话题 - cpu 多线程和cuda(device)的结合。 http://stackoverflow.com/questions/14474869/call-cuda-kernel-from-a-multithreaded-c-application
 
 与cudaOpenMP中不同的是，cudaOpenMP是#pragma omp parallel 同构的，而我们要处理更复杂的情形 - 异构。
 
 
 
 
1.关于 typedef
	http://niehan.blog.techweb.com.cn/archives/325.html
	不少人用#define的思维来看待typedef，把int与PARA分开来看，int是一部分，PARA是另一部分，但实际上根本就不是这么一回事。int与PARA是一个整体!就象int i:声明一样是一个整体声明，只不过int i定义了一个变量，而typedef定义了一个别名。
	
	这些人由于持有这种错误的观念，就会无法理解如下一些声明：typedef int a[10]; typedef void (*p)(void); 他们会以为a[10]是int的别名，(*p)(void)是void的别名，但这样的别名看起来又似乎不是合法的名字，于是陷入困惑之中。实际上，上面的语句把a声明为具有10个int元素的数组的类型别名，p是一种函数指针的类型别名。 虽然在功能上，typedef可以看作一个跟int PARA分离的动作，但语法上typedef属于存储类声明说明符，因此严格来说，typedef int PARA整个是一个完整的声明。 

	（自注；就是用占位符（变量）表示占位符（变量）当前的类型
	
	经典的struct为例，
		typedef struct tagPOINT{
		　　int x;
		　　int y;
		}POINT;

	看似声明一个变量，实则声明一种类型。

2. 关于 void*
	http://stackoverflow.com/questions/5104640/what-is-void-and-to-what-variables-objects-it-can-point-to
	
	void* is such a pointer, that any pointer can be implicitly converted to void*
	Also note that pointers to const cannot be converted to void* without a const_cast

解释如下：

	In C any pointer can point to any address in memory, as the type information is in the pointer, not in the target. So an int* is just a pointer to some memory location, which is believed to be an integer. A void* pointer, is just a pointer to a memory location where the type is not defined (could be anything).

	Thus any pointer can be cast to void*, but not the other way around, because casting (for example) a void pointer to an int pointer is adding information to it - by performing the cast, you are declaring that the target data is integer, so naturally you have to explicitly say this. The other way around, all you are doing is saying that the int pointer is some kind of pointer, which is fine.

	(If I am not mistaken, in C, unlike C++ the conversion from void* to int* is implicit too...)



3.cudaStreamAddCallback

extern __host__ cudaError_t CUDARTAPI cudaStreamAddCallback(cudaStream_t stream,
        cudaStreamCallback_t callback, void *userData, unsigned int flags);

4.关于concurrent kernels
	http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution
	
	BLOG ： How to Overlap Data Transfers in CUDA C/C++
	http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/


	cudaMemcpy(d_a, a, numBytes, cudaMemcpyHostToDevice);
	increment<<<1,N>>>(d_a)
	myCpuFunction(b)
	cudaMemcpy(a, d_a, numBytes, cudaMemcpyDeviceToHost);

	注意：上述代码中的第3行，host端并没有被kernel阻塞！(?)

5.call back - cpu

	// New in CUDA 5.0: Add a CPU callback which is called once all currently pending operations in the CUDA stream have finished
	checkCudaErrors(cudaStreamAddCallback(workload->stream, myStreamCallback, workload, 0));

	原型：
	extern __host__ cudaError_t CUDARTAPI cudaStreamAddCallback(cudaStream_t stream,
        cudaStreamCallback_t callback, void *userData, unsigned int flags);
	
		cudaStreamAddCallback call, a callback will be executed exactly once.
	 * The callback will block later work in the stream until it is finished.
	 *
	 * The callback may be passed ::cudaSuccess or an error code.  In the event
	 * of a device error, all subsequently executed callbacks will receive an
	 * appropriate ::cudaError_t.
	 *
	 * Callbacks must not make any CUDA API calls.  Attempting to use CUDA APIs
	 * will result in ::cudaErrorNotPermitted.  Callbacks must not perform any
	 * synchronization that may depend on outstanding device work or other callbacks
	 * that are not mandated to run earlier.  Callbacks without a mandated order
	 * (in independent streams) execute in undefined order and may be serialized.
	 
	例如：
		void CUDART_CB myStreamCallback(cudaStream_t stream, cudaError_t status, void *data) {
			checkCudaErrors(status);
			cutStartThread(postprocess, data);
		}

	又例：
		http://stackoverflow.com/questions/17943295/how-to-implement-cudastreamaddcallback
		You pass a class method to cudaStreamAddCallback, but it should be a non-member function (global or static). If you want to use class method you should implement wrapper function that will call the method
	
	由上可知，callback函数的条件比较严格。
	
	
		
6.(补充) Pthread：POSIX 多线程程序设计
	http://www.cnblogs.com/mywolrd/archive/2009/02/05/1930707.html

	独立的控制流得以实现是因为线程维持着自己的： 
		堆栈指针 
		寄存器 
		调度属性（如：策略或优先级） 
		待定的和阻塞的信号集合（Set of pending and blocked signals） 
		线程专用数据（TSD：Thread Specific Data.） 
	因此，在UNIX环境下线程： 
		存在于进程，使用进程资源 
		拥有自己独立的控制流，只要父进程存在并且操作系统支持 
		只复制必可以使得独立调度的必要资源 
		可以和其他线程独立（或非独立的）地共享进程资源 
		当父进程结束时结束，或者相关类似的 
		是“轻型的”，因为大部分额外开销已经在进程创建时完成了 
	
	“考虑在SMP架构上使用Pthreads的主要动机是获的最优的性能。特别的，如果一个程序使用MPI在节点通信，使用Pthreads可以使得节点数据传输得到显著提高。”


	https://computing.llnl.gov/tutorials/parallel_comp/

	互斥量（Mutex）是“mutual exclusion”的缩写。互斥量是实现线程同步，和保护同时写共享数据的主要方法.


	unistd.h 是 C 和 C++ 程序设计语言中提供对 POSIX 操作系统 API 的访问功能的头文件的名称。该头文件由 POSIX.1 标准（单一UNIX规范的基础）提出，故所有遵循该标准的操作系统和编译器均应提供该头文件（如 Unix 的所有官方版本，包括 Mac OS X、Linux 等）。
	对于类 Unix 系统，unistd.h 中所定义的接口通常都是大量针对系统调用的封装（英语：wrapper functions），如 fork、pipe 以及各种 I/O 原语（read、write、close 等等）。
	类似于 Cygwin 和 MinGW 的 Unix 兼容层也提供相应版本的 unistd.h。

	调用pthread_cond_wait()阻塞等待Thread-B的信号。注意pthread_cond_wait()能够自动地并且原子地解锁相关的互斥量，以至于它可以被Thread-B使用。
	当收到信号，唤醒线程，互斥量被自动，原子地锁定。(释放锁很容易，要得到的话就得排队)

7.补充：关于命令time
	
	The default format string is 
		\n%U-user %S-system %e-elapsed %P-CPU (%Xtext+%Ddata %Mmax)k\n%Iinputs+%Ooutputs (%Fmajor+%Rminor)pagefaults %Wswaps\n
		real %es\nuser %Us\nsys %Ss\nCPU %P\n(%Xtext+%Ddata %Mmax)k %Iinputs+%Ooutputs\n(%Fmajor+%Rminor)pagefaults %Wswaps
	When the -p option is given the (portable) output format 
		real %e 
		user %U 
		sys %S 
	is used.  	
	/usr/bin/time -f "\n%E elapsed,\n%U user,\n%S system,\n%M memory\n%x status" 

	%P - Percentage of the CPU that this job got, computed as (%U + %S) / %E.


	http://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1

	Real, User and Sys process time statistics

	One of these things is not like the other. Real refers to actual elapsed time; User and Sys refer to CPU time used only by the process.

	Real is wall clock time - time from start to finish of the call. This is all elapsed time including time slices used by other processes and time the process spends blocked (for example if it is waiting for I/O to complete).

	User is the amount of CPU time spent in user-mode code (outside the kernel) within the process. This is only actual CPU time used in executing the process. Other processes and time the process spends blocked do not count towards this figure.

	Sys is the amount of CPU time spent in the kernel within the process. This means executing CPU time spent in system calls within the kernel, as opposed to library code, which is still running in user-space. Like 'user', this is only CPU time used by the process. See below for a brief description of kernel mode (also known as 'supervisor' mode) and the system call mechanism.

	注意：User+Sys will tell you how much actual CPU time your process used. Note that this is across all CPUs, so if the process has multiple threads it could potentially exceed the wall clock time reported by Real. 

	The statistics reported by time are gathered from various system calls. 'User' and 'Sys' come from wait (2) or times (2), depending on the particular system. 'Real' is calculated from a start and end time gathered from the gettimeofday (2) call. （注意：来自gettimeofday）


//0918

/**
 * 
 * simpleCubemapTexture
 * 
 * Simple example that demonstrates how to use a new CUDA 4.1 feature to support cubemap Textures in CUDA C.
 * 
 */
/*
 * This sample demonstrates how to use texture fetches from layered 2D textures in CUDA C
 *
 * This sample first generates a 3D input data array for the layered texture
 * and the expected output. Then it starts CUDA C kernels, one for each layer,
 * which fetch their layer's texture data (using normalized texture coordinates)
 * transform it to the expected output, and write it to a 3D output data array.
 */
-1.关于constant memory
	http://cuda-programming.blogspot.in/2013/01/what-is-constant-memory-in-cuda.html
	constant memory - NVIDIA hardware provides 64KB of constant memory that it treats differently than it treats standard global memory. In some situations, using constant memory rather than global memory will reduce the required memory bandwidth.

	How does Constant memory speed up your CUDA code performance?

	There is a total of 64 KB constant memory on a device. The constant memory space is cached. As a result, a read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache.
	
	Advantage along with disadvantage
	
	For all threads of a half warp, reading from the constant cache is as fast as reading from a register as long as all threads read the same address. Accesses to different addresses by threads within a half warp are serialized, so cost scales linearly with the number of different addresses read by all threads within a half warp.

	（这是什么意思？这为什么和global mem的效果相反？是因为constant mem（cache）只有一个bank吗？）
	
		Step 1: A constant memory request for a warp is first split into two requests, one for each half-warp, that are issued independently.

		Step 2: A request is then split into as many separate requests as there are different memory addresses in the initial request, decreasing throughput by a factor equal to the number of separate requests.

		Final Step: The resulting requests are then serviced at the throughput of the constant cache in case of a cache hit, or at the throughput of device memory otherwise.

	（然后作者给出了这样的建议-）
		Use Constant memory in the following cases
			o   When you know, your input data will not change during the execution
			o   When you know, your all thread will access data from same part of memory
		As shown in above example; every thread in a block access same address space pointed out by “cangle” array.
		
		
-2.带宽测量
	http://cuda-programming.blogspot.in/2013/01/how-to-implement-performance-metrics-in.html
	BW_Theoretical = 1546 * 106 * (384/8) * 2 / 109 = 148 GB/s
	BW_Effective = (RB + WB) / (t * 109)		
		
-3.关于线性存储器 及 pitch 对齐
	http://blog.sina.com.cn/s/blog_5c6f793801018ekh.html

		
		
0.关于CUDA array
	http://cuda-programming.blogspot.com/2013/02/cuda-array-in-cuda-how-to-use-cuda.html
	
	
	
1.CUDA 纹理内存
	从函数cudaBindTextureToArray可以看出，texture只是一个cache，需要绑定到内存。cache和内存的区别是不在一个层次上，cache是活的。
	
	那么constant呢？？？怎么实现的？
	

参考 - 
	http://www.cnblogs.com/traceorigin/archive/2013/04/11/3015755.html
	《CUDA范例精解通用GPU编程》 chap7
	NVIDIA_CUDA_Programming_Guide_1.1_chs  chap4.3.4 纹理类型 
	CUDA_C_Programming_Guide v6.5  chap3.2.11. Texture and Surface Memory
	http://cuda-programming.blogspot.in/2013/02/texture-memory-in-cuda-what-is-texture.html

	Specifically, texture caches are designed for graphics applications where memory access patterns exhibit a great deal of spatial locality. In a computing application, this roughly implies that a thread is likely to read from an address “near” the address that nearby threads read, as shown in Figure

	Arithmetically, the four addresses shown are not consecutive, so they would not be cached together in a typical CPU caching scheme. But since GPU texture caches are designed to accelerate access patterns such as this one, you will see an increase in performance in this case when using texture memory instead of global memory.

	(所以，不同种类的特殊memory主要是面向不同的访问模式(MAP,memory access pattern))

	 The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance.


2.关于Driver API 和 Runtime API
	http://bbs.csdn.net/topics/310181849
	In most cases, these points tend to steer developers strongly toward one API. In cases where they do not, favor the runtime API because it is higher level and easier to use. In addition, because runtime functions all have driver API equivalents, it is easy to migrate runtime code to the driver API should that later become necessary.

	“嗯，貌似这样搞两种API 显得乱得很，毕竟这个东西还不像 WIN32 SDK 和MFC 关系那样容易理解。” （？？？）

	
	http://cuda-programming.blogspot.com/2013/01/what-is-cuda-driver-api-and-cuda.html

	The CUDA runtime makes it possible to compile and link your CUDA kernels into executable.This means that you don't have to distribute Cubin files with your application, or deal with loading them through the driver API. As you have noted, it is generally easier to use.

//0920

/**
 * 
 * simpleIPC
 * 
 * This sample demonstrates Inter Process Communication
 * 
 */
 
 《CUDA_C_Programming_Guide》
 3.2.8. Interprocess Communication
	Any device memory pointer or event handle created by a host thread can be directly
	referenced by any other thread within the same process. It is not valid outside this
	process however, and therefore cannot be directly referenced by threads belonging to a
	different process.

	To share device memory pointers and events across processes, an application must use the Inter Process Communication API
-1.插一句：关于x86_64 和 amd64
	http://en.wikipedia.org/wiki/X86-64
	
	"AMD64" and "Intel 64" redirect here. For the Intel 64-bit architecture called IA-64, see Itanium.

	x86-64 (also known as x64, x86_64 and AMD64) is the 64-bit version of the x86 instruction set. It supports vastly larger amounts (theoretically, 264 bytes or 16 exbibytes) of virtual memory and physical memory than is possible on its predecessors, allowing programs to store larger amounts of data in memory. x86-64 also provides 64-bit general purpose registers and numerous other enhancements. The original specification was created by AMD, and has been implemented by AMD, Intel, VIA, and others.
	
	x86 is a family of backward compatible instruction set architectures[a] based on the Intel 8086 CPU. Many additions and extensions have been added to the x86 instruction set over the years, almost consistently with full backward compatibility.[b] The architecture has been implemented in processors from Intel, Cyrix, AMD, VIA and many other companies; there are also open implementations, such as the Zet SoC platform.
	
0.关于进程间通信

	剪贴板就是最简单的进程间通信的例子。又如ctrl-c
	http://baike.baidu.com/view/1492468.htm?fr=aladdin
	进程的用户空间是互相独立的，一般而言是不能互相访问的，唯一的例外是共享内存区。但是，系统空间却是“公共场所”，所以内核显然可以提供这样的条件。除此以外，那就是双方都可以访问的外设了。在这个意义上，两个进程当然也可以通过磁盘上的普通文件交换信息，或者通过“注册表”或其它数据库中的某些表项和记录交换信息。广义上这也是进程间通信的手段，但是一般都不把这算作“进程间通信”。

	进程间通信主要包括管道, 系统IPC(Inter-Process Communication，进程间通信)(包括消息队列,信号,共享存储), 套接字(SOCKET).

	在 Linux 中，信号的种类和数目与硬件平台有关。内核用一个字代表所有的信号，每个信号占一位，因此一个字的位数就是系统可以支持的最多信号种类数。i386 平台上有31 种信号。可以使用kill命令（kill –l）列出系统中的信号集。下面是Linux 在Intel系统中的信号。
	
	HUP INT QUIT ILL TRAP ABRT BUS FPE KILL USR1
	SEGV USR2 PIPE ALRM TERM STKFLT CHLD CONT STOP TSTP
	TTIN TTOU URG XCPU XFSZ VTALRM PROF WINCH POLL PWR SYS
	
	SIGBUS和SIGSEGV 
	http://blog.chinaunix.net/uid-24599332-id-2122898.html
	
	
	 消息缓冲通信技术是由Hansen首先提出的,其基本思想是:根据”生产者-消费者”原理,利用内存中公用消息缓冲区实现进程之间的信息交换.  

	Linux下的进程间通信-详解
	http://blog.csdn.net/eroswang/article/details/1772350

	共享内存是运行在同一台机器上的进程间通信最快的方式，因为数据不需要在不同的进程间复制。通常由一个进程创建一块共享内存区，其余进程对这块内存区进行 读写。得到共享内存有两种方式：映射/dev/mem设备和内存映像文件。
	 int shmget(key_t key, int size, int flag); 
	 void *shmat(int shmid, void *addr, int flag); 
	使用共享存储来实现进程间通信的注意点是对数据存取的同步，必须确保当一个进程去读取数据时，它所想要的数据已经写好了。通常，信号量被要来实现对共享存 储数据存取的同步，另外，可以通过使用shmctl函数设置共享存储内存的某些标志位如SHM_LOCK、SHM_UNLOCK等来实现。 


1.编译制导 #if
	inline bool IsAppBuiltAs64() {
#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)
		return 1;
#else
		return 0;
#endif
	}

2.mmap

	void *mmap (void *__addr, size_t __len, int __prot, int __flags, int __fd, __off_t __offset)
	mmap将一个文件或者其它对象映射进内存。文件被映射到多个页上，如果文件的大小不是所有页的大小之和，最后一个页不被使用的空间将会清零。mmap在用户空间映射调用系统中作用很大。
	
	
	http://kenby.iteye.com/blog/1164700
		共享内存可以说是最有用的进程间通信方式，也是最快的IPC形式, 因为进程可以直接读写内存，而不需要任何
	数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则
	只拷贝两次数据: 一次从输入文件到共享内存区，另一次从共享内存区到输出文件。实际上，进程之间在共享内
	存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直
	到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映
	射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。
	
	mmap的作用是映射文件描述符fd指定文件的 [off,off + len]区域至调用进程的[addr, addr + len]的内存区域。
	
	.
	.
	.
	/* 将文件映射至进程的地址空间 */  
    if ((mapped = (char *)mmap(NULL, sb.st_size, PROT_READ |   
                    PROT_WRITE, MAP_SHARED, fd, 0)) == (void *)-1) {  
        perror("mmap");  
    }  
    
	/* 映射完后, 关闭文件也可以操纵内存 */  
    close(fd);  
  
    printf("%s", mapped);  
  
    /* 修改一个字符,同步到磁盘文件 */  
    mapped[20] = '9';  
    if ((msync((void *)mapped, sb.st_size, MS_SYNC)) == -1) {  
        perror("msync");  
    }  
  
    /* 释放存储映射区 */  
    if ((munmap((void *)mapped, sb.st_size)) == -1) {  
        perror("munmap");  
    }  
	.
	.
	.
	
	私有映射无法修改文件
	/* 将文件映射至进程的地址空间 */
	if ((mapped = (char *)mmap(NULL, sb.st_size, PROT_READ | 
						PROT_WRITE, MAP_PRIVATE, fd, 0)) == (void *)-1) {
		perror("mmap");
	}

	通过匿名映射实现父子进程通信:

	/* 匿名映射,创建一块内存供父子进程通信 */  
    p_map = (char *)mmap(NULL, BUF_SIZE, PROT_READ | PROT_WRITE,  
            MAP_SHARED | MAP_ANONYMOUS, -1, 0);  
  
    if(fork() == 0) {  
        sleep(1);  
        printf("child got a message: %s\n", p_map);  
        sprintf(p_map, "%s", "hi, dad, this is son");  
        munmap(p_map, BUF_SIZE); //实际上，进程终止时，会自动解除映射。  
        exit(0);  
    }  
    
3.spawn  vt. vi.（鱼、蛙等）大量产（卵）;引起，酿成	vt.	大量生产
4.prop.unifiedAddressing /**< Device shares a unified address space with the host */

5.UVA
	http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-virtual-address-space
	http://streamcomputing.eu/blog/2013-11-14/cuda-6-unified-memory-explained/
	CUDA 6 Unified Memory explained

	TRUE or VIRTUAL ?

	AMD, ARM-vendors and Intel have been busy unifying CPU and GPU memories for years. It is not easy to design a model where 2 (or more) processors can access memory without dead-locking each other.

	So if it is not unified memory, what is it?

	It is intelligent synchronisation between CPU and GPU-memory. The real question is what the difference is between Unified Virtual Addressing (UVA, introduced in CUDA 4) and this new thing.

	UVA defines a single Address Space, where CUDA takes care of  the synchronisation when the addresses are physically not on the same memory space. The developer has to give ownership to or the CPU or the GPU, so CUDA knows when to sync memories. It does need CudeDeviceSynchronize() to trigger synchronisation (see image).

	This in turn is intended to make CUDA programming more accessible to wider audiences that may not have been interested in doing their own memory management, or even just freeing up existing CUDA developers from having to do it in the future, speeding up code development.

	“I don’t bike on highways.”
	“It simply matters a lot if it’s true or virtual, and I really don’t understand why NVIDIA chose to obfuscate these matters.”

	comment: What you have under "official definition" above seems to actually be for "Uniform Memory Access". But CUDA uses the term "Unified Memory". These are different terms, so I don't think it's super misleading on nVidia's part -- I have never seen nVidia actually use the term UMA with this. Though I agree UVM would have been a better term than UM.

6.在UVA之前应该谈谈CUDA6的UM

	（关于统一内存访问，该话题打开了新世界的大门。。。）

	Unified Memory in CUDA 6
	http://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/
	
	With CUDA 6, we’re introducing one of the most dramatic programming model improvements in the history of the CUDA platform, Unified Memory.
	
	Unified Memory creates a pool of managed memory that is shared between the CPU and GPU, bridging the CPU-GPU divide. Managed memory is accessible to both the CPU and GPU using a single pointer. The key is that the system automatically migrates data allocated in Unified Memory between host and device so that it looks like CPU memory to code running on the CPU, and like GPU memory to code running on the GPU.
	
	If you have programmed CUDA C/C++ before, you will no doubt be struck by the simplicity of the code on the right. Notice that we allocate memory once, and ... ... The CUDA runtime hides all the complexity, automatically migrating data to the place where it is accessed.
	
	By migrating data on demand between the CPU and GPU, Unified Memory can offer the performance of local data on the GPU, while providing the ease of use of globally shared data. The complexity of this functionality is kept under the covers of the CUDA driver and runtime, ensuring that application code is simpler to write. The point of migration is to achieve full bandwidth from each processor; the 250 GB/s of GDDR5 memory is vital to feeding the compute throughput of a Kepler GPU.
	
	Unified Memory or Unified Virtual Addressing?（UM or UVA）
	
		CUDA has supported Unified Virtual Addressing (UVA) since CUDA 4, and while Unified Memory depends on UVA, they are not the same thing. UVA provides a single virtual memory address space for all memory in the system, and enables pointers to be accessed from GPU code no matter where in the system they reside, whether its device memory (on the same or a different GPU), host memory, or on-chip shared memory. It also allows cudaMemcpy to be used without specifying where exactly the input and output parameters reside. UVA enables “Zero-Copy” memory, which is pinned host memory accessible by device code directly, over PCI-Express, without a memcpy. Zero-Copy provides some of the convenience of Unified Memory, but none of the performance, because it is always accessed with PCI-Express’s low bandwidth and high latency.

		UVA does not automatically migrate data from one physical location to another, like Unified Memory does. Because Unified Memory is able to automatically migrate data at the level of individual pages between host and device memory, it required significant engineering to build, since it requires new functionality in the CUDA runtime, the device driver, and even in the OS kernel. 
	
	
7.课外：高级体系结构
	《ADVANCED COMPUTER ARCHITECTURE AND PARALLEL PROCESSING》
	http://site.ebrary.com/lib/tsinghua/docDetail.action?docID=10114055&ppg=1



//0925

/**
 * 
 * simpleMPI
 * 
 * Simple example demonstrating how to use MPI in combination with CUDA.  This executable is not pre-built with the SDK installer.
 * 
 */

➜ chunk@chunk-precise  ~/samples/0_Simple/simpleMPI  make

/usr/local/bin/mpicxx -I../../common/inc    -o simpleMPI_mpi.o -c simpleMPI.cpp
"/usr/local/cuda-6.0"/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_10,code=sm_10 -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_32,code=sm_32 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_50,code=compute_50 -o simpleMPI.o -c simpleMPI.cu
nvcc warning : The 'compute_10' and 'sm_10' architectures are deprecated, and may be removed in a future release.
/usr/local/bin/mpicxx    -o simpleMPI simpleMPI_mpi.o simpleMPI.o  -L"/usr/local/cuda-6.0"/lib64 -lcudart
mkdir -p ../../bin/x86_64/linux/release
cp simpleMPI ../../bin/x86_64/linux/release

/**
 * 
 * simpleTexture
 * 
 * Simple example that demonstrates use of Textures in CUDA.
 * 
 */

1.pgm格式和ppm格式

	PGM 是便携式灰度图像格式(portable graymap file format),在黑白超声图像系统中经常使用PGM格式的图像.文件的后缀名为".pgm",PGM格式图像格式分为两类:P2和P5类型.不管是P2还是P5类型的PGM文件,都由两部分组成,文件头部分和数据部分.

	文件头部分
	文件头包括的信息依次是:
	1.PGM文件的格式类型(是P2还是P5);
	2.图像的宽度;
	3.图像的高度;
	4.图像灰度值可能的最大值;
	文件头的这四部分信息都是以ASCII码形式存储的,所以可以直接在将P2或P5格式的PGM文件在记事本中打开看到文件头的信息.

	数据部分
	数据部分记录图像每个像素的灰度值,按照图像从上到下,从左到右的顺序依次存储每个像素的灰度值.对于像素灰度值的表示P2格式和P5格式有所不同.

	PPM（Portable PixMap）是portable像素图片，是有netpbm项目定义的一系列的portable图片格式中的一个。这些图片格式都相对比较容易处理，跟平台无关，所以称之为portable，简单理解，就是比较直接的图片格式，比如PPM，其实就是把每一个点的RGB分别保存起来。所以，PPM格式的文件是没有压缩的，相对比较大，但是由于图片格式简单，一般作为图片处理的中间文件（不会丢失文件信息），或者作为简单的图片格式保存。


//0927
/**
 * 
 * 5_workbench
 * 
 * 
This is a simple test program to measure the memcopy bandwidth of the GPU and memcpy bandwidth across PCI-e.  This test application is capable of measuring device to device copy bandwidth, host to device copy bandwidth for pageable and page-locked memory, and device to host copy bandwidth for pageable and page-locked memory.
 * 
 */
今天我们进入第二层次的samples。首先仿写一个bench程序，从bandwidth开始。


1.memory mode
	pinned memory 
	http://www.cs.virginia.edu/~mwb7w/cuda_support/pinned_tradeoff.html
	
	When allocating CPU memory that will be used to transfer data to the GPU, there are two types of memory to choose from: pinned and non-pinned memory. Pinned memory is memory allocated using the cudaMallocHost function, which prevents the memory from being swapped out and provides improved transfer speeds. Non-pinned memory is memory allocated using the malloc function. 

	pinned memory is much more expensive to allocate and deallocate but provides higher transfer throughput for large memory transfers. 


2.关于cudaHostAlloc的flag
	之前我们提到过cudaHostAlloc与cudaMallocHost的区别在于前者可以指定flag。
	#define cudaHostAllocWriteCombined     0x04  /**< Write-combined memory */

	Write combining
	http://en.wikipedia.org/wiki/Write_combining

	Write combining (WC[1]) is a computer bus technique for allowing data to be combined and temporarily stored in a buffer — the write combine buffer (WCB) — to be released together later in burst mode instead of writing (immediately) as single bits or small chunks.

	Write combining cannot be used for general memory access (data or code regions) due to the weak ordering. Write-combining does not guarantee that the combination of writes and reads is done in the expected order. For example, a Write/Read/Write combination to a specific address would lead to the write combining order of Read/Write/Write which can lead to obtaining wrong values with the first read (which potentially relies on the write before).

	In order to avoid the problem of read/write order described above, the write buffer can be treated as a fully associative cache and added into the memory hierarchy of the device in which it is implemented.[2] Adding complexity slows down the memory hierarchy so this technique is often only used for memory which does not need strong ordering (always correct) like the frame buffers of video cards.

	参考 http://download.intel.com/design/PentiumII/applnots/24442201.pdf

	Write combining is not your friend
	http://fgiesen.wordpress.com/2013/01/29/write-combining-is-not-your-friend/



/**
 * 
 * 6_cuGL
 * 
 */
今天我们进入第三层次的samples：graphics。接下来一组重要的程序是cuda+openGL。



/**
 * 
 * simpleGL
 * 
 * Simple program which demonstrates interoperability between CUDA and OpenGL. The program modifies vertex positions with CUDA and uses OpenGL to render the geometry.
 * 
 */

0.about openGL

	OpenGL is state-based.
	
	http://web.cs.wpi.edu/~matt/courses/cs563/talks/OpenGL_Presentation/OpenGL_Presentation.html

	http://www.nvidia.com/content/gtc/documents/1055_gtc09.pdf

	Vertices may be specified in 2D, 3D, or 4D. 2D coordinates are promoted to 3D by assigning a Z value of zero. 4D homogeneous coordinates are reduced to 3D by dividing x, y, and z by the w coordinate (if non-zero).

	Optional vertex attributes are picked up from state if not specified per-vertex. The normal is a 3D vector perpendicular to the surface being described, and is used during lighting calculations. The color may be an RGBA value, or a Color Index, depending on the visual type of the window. Texture coordinates determine the mapping of a texture onto the vertex, and may be specified with 1, 2, 3, or 4 parameters. Edge flags are used to specify if the vertex is on a boundary of a surface. Material properties specify such things as reflectance, ambience, etc, and are used for lighting calculations.


	- cudaGLRegisterBufferObject( GLuint bufObj );
	- cudaGLUnregisterBufferObject( GLuint bufObj );
		These commands simply inform the OpenGL and CUDA drivers that this buffer will be used by both

	Provides a CUDA pointer to the GL buffer─on 
	a single GPU no data is moved (Win & Linux)
	• When mapped to CUDA, OpenGL should not use 
	this buffer

	• CUDA C kernels may now use the mapped memory 
	just like regular GMEM
	• CUDA copy functions can use the mapped memory 
	as a source or destination













































































































































































































































